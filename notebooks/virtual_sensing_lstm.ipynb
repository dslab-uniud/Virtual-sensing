{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "################# Data Loading and Preprocessing\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataframe from disk, if it exists\n",
    "\n",
    "try:\n",
    "    pickle_out_extended_dataframe = open('extended_dataframe.pickle',\"rb\")\n",
    "    data = pickle.load(pickle_out_extended_dataframe)\n",
    "    pickle_out_extended_dataframe.close()\n",
    "except IOError:\n",
    "    print(\"No previous dataframe is available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I delete all days containing NaN values\n",
    "# Better to do proceed in this way, so not to have strange cut points\n",
    "\n",
    "rows_with_nans = np.where(np.isnan(data['temperature'].values))[0]\n",
    "dates_with_nans = np.unique(data['only_date'][rows_with_nans])\n",
    "dates_indexes = data.index[data['only_date'].isin(dates_with_nans)].tolist()\n",
    "print(\"% of discarded data:\", len(dates_indexes) / len(data))\n",
    "data = data.drop(data.index[dates_indexes]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year_week'] = data.datetime.dt.strftime('%Y-%U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sensor, I determine the indexes to partition its data into training and test\n",
    "# In doing that, I keep into account the temporal relationships\n",
    "# Meaning, test data is in the future wrt training data\n",
    "\n",
    "perc_training = 0.8\n",
    "\n",
    "\n",
    "split_type = 'week_based_random' # temporal_wise, day_based_random, full_random\n",
    "\n",
    "    \n",
    "# Checking that every sensor has the same length\n",
    "counts_per_sensor = data[['node', 'datetime']].groupby('node').count()\n",
    "assert np.max(counts_per_sensor['datetime']) == np.min(counts_per_sensor['datetime'])\n",
    "sensor_data_length = counts_per_sensor['datetime'][0]    \n",
    "\n",
    "   \n",
    "# Maps that store the training and test indexes for each sensor\n",
    "sens_map_train_indexes = {}\n",
    "sens_map_test_indexes = {}\n",
    "\n",
    "for sens in list(counts_per_sensor.index):\n",
    "    sens_map_train_indexes[sens] = []\n",
    "    sens_map_test_indexes[sens] = []    \n",
    "    \n",
    "    \n",
    "    \n",
    "if split_type == 'temporal_wise':\n",
    "    # first 70% of each sensor data is traning, and next 30% is test\n",
    "\n",
    "    # Generate training and test indexes\n",
    "\n",
    "    # I have already sorted the values by node and datetime in the original dataframe\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i*sensor_data_length\n",
    "        last_train = base_sens + int(sensor_data_length*perc_training)\n",
    "        last_test = last_train + int(sensor_data_length*(1 - perc_training)) + 1\n",
    "        train_inds = list(range(base_sens, last_train))\n",
    "        test_inds = list(range(last_train, last_test))\n",
    "        sens_map_train_indexes[sens] = train_inds\n",
    "        sens_map_test_indexes[sens] = test_inds\n",
    "        \n",
    "    for sens in list(counts_per_sensor.index):\n",
    "        first_train = sens_map_train_indexes[sens][0]\n",
    "        last_train = sens_map_train_indexes[sens][-1]\n",
    "        first_test = sens_map_test_indexes[sens][0]\n",
    "        last_test = sens_map_test_indexes[sens][-1]\n",
    "        assert first_train == data.query(\"node == @sens\").reset_index().iloc[0,0]\n",
    "        assert last_test == data.query(\"node == @sens\").reset_index().iloc[-1,0]\n",
    "        assert last_train > first_train and last_train < first_test\n",
    "        assert first_test > last_train and first_test < last_test\n",
    "    \n",
    "elif split_type == 'full_random':\n",
    "    # divide randomly the data of the sensors\n",
    "    \n",
    "    possible_indexes = list(range(sensor_data_length))\n",
    "    random.seed(42)\n",
    "    # I make sure to use the same split for each sensor\n",
    "    # meaning, corresponding training and test indexes of the different sensors refer to the same datetimes\n",
    "    train_indexes_overall = sorted(random.sample(possible_indexes, int(sensor_data_length * perc_training)))\n",
    "    test_indexes_overall = sorted(list(set(possible_indexes) - set(train_indexes_overall)))\n",
    "    \n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i * sensor_data_length\n",
    "        # it is sufficent to add the baseline, since all sensor tracks have the same length\n",
    "        sens_map_train_indexes[sens] = np.asarray(train_indexes_overall) + base_sens\n",
    "        sens_map_test_indexes[sens] = np.asarray(test_indexes_overall) + base_sens\n",
    "\n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(possible_indexes)\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'day_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    days_list = data.only_date.unique().tolist()\n",
    "    np.random.shuffle(days_list)\n",
    "    random_train_days = sorted(days_list[0:int(len(days_list)*perc_training)])\n",
    "    random_test_days =  sorted(days_list[int(len(days_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.only_date.isin(random_train_days)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.only_date.isin(random_test_days)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'week_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    weeks_list = data.year_week.unique().tolist()\n",
    "    np.random.shuffle(weeks_list)\n",
    "    random_train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc_training)])\n",
    "    random_test_weeks =  sorted(weeks_list[int(len(weeks_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.year_week.isin(random_train_weeks)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.year_week.isin(random_test_weeks)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "else:\n",
    "    assert False, 'Unsupported split type'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general information\n",
    "\n",
    "window_coords = [(1, 0), (3, 0), (6, 0), (8, 0), (10, 0),\n",
    "                 (1, 9), (3, 9), (6, 9), (8, 9), (10, 9)]\n",
    "\n",
    "all_train_idxs = []\n",
    "for sens in sens_map_train_indexes:\n",
    "    all_train_idxs.extend(sens_map_train_indexes[sens])\n",
    "\n",
    "all_test_idxs = []\n",
    "for sens in sens_map_test_indexes:\n",
    "    all_test_idxs.extend(sens_map_test_indexes[sens])\n",
    "\n",
    "node_map_coords = {}\n",
    "for sens in sens_map_test_indexes:\n",
    "    coord_x, coord_y = data[data['node'] == sens].iloc[0][['coord_x', 'coord_y']].values\n",
    "    node_map_coords[sens] = (coord_x, coord_y)\n",
    "    \n",
    "all_sensors = np.unique(data['node'])\n",
    "\n",
    "all_training_data = data.iloc[all_train_idxs].reset_index(drop=True)\n",
    "all_test_data = data.iloc[all_test_idxs].reset_index(drop=True)\n",
    "\n",
    "def eucl_dist(x_0, y_0, x_1, y_1):\n",
    "    return np.sqrt((x_0 - x_1)**2 + (y_0 - y_1)**2)\n",
    "\n",
    "training_data_ts = len(all_training_data[all_training_data['node'] == 'raspihat01'])\n",
    "\n",
    "test_data_ts = len(all_test_data[all_test_data['node'] == 'raspihat01'])\n",
    "\n",
    "ref_sensors = ['raspihat02','raspihat03','raspihat05']\n",
    "\n",
    "air_tube_coords = [(x, 4) for x in list(range(11))]\n",
    "\n",
    "def array_rep(arr, n):\n",
    "    assert len(arr.shape) <= 4\n",
    "    if len(arr.shape) == 1:\n",
    "        return np.tile(arr, n)\n",
    "    elif len(arr.shape) == 2:\n",
    "        retarr = np.full((arr.shape[0]*n, arr.shape[1]), -1.)\n",
    "        for i in range(n):\n",
    "            retarr[i*arr.shape[0]: (i+1)*arr.shape[0]] = arr\n",
    "        return retarr\n",
    "    elif len(arr.shape) == 3:\n",
    "        retarr = np.full((arr.shape[0]*n, arr.shape[1], arr.shape[2]), -1.)\n",
    "        for i in range(n):\n",
    "            retarr[i*arr.shape[0]: (i+1)*arr.shape[0]] = arr\n",
    "        return retarr\n",
    "    elif len(arr.shape) == 4:\n",
    "        retarr = np.full((arr.shape[0]*n, arr.shape[1], arr.shape[2], arr.shape[3]), -1.)\n",
    "        for i in range(n):\n",
    "            retarr[i*arr.shape[0]: (i+1)*arr.shape[0]] = arr\n",
    "        return retarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Weighted distance utils functions\n",
    "\n",
    "## Spatial distances \n",
    "center = (5,5)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## Spatial distances \n",
    "\n",
    "# Angular distance between two point\n",
    "def angular_distance(p1,p2):\n",
    "    # determines the angle between two points (x_0, y_0) (x_1, y_1)\n",
    "    # if p2 is the origin (0, 0), then I get the angle wrt x axis\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "\n",
    "    ang1 = np.arctan2(p1[:,1], p1[:,0])\n",
    "    ang2 = np.arctan2(p2[:,1], p2[:,0])\n",
    "    \n",
    "    return np.minimum(np.rad2deg(np.mod(ang1 - ang2, 2 * np.pi)), np.rad2deg(np.mod(ang1 - ang2, 2 * np.pi)))\n",
    "    \n",
    "# Euclidean distance between two point\n",
    "def euclidean_distance(p1,p2):    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "    return np.sqrt(np.add(np.subtract(p1[:,0],p2[:,0])**2, np.subtract(p1[:,1],p2[:,1])**2))\n",
    "\n",
    "# Manhattan distance between two point\n",
    "def manhattan_distance(p1,p2):\n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])    \n",
    "    return np.abs(p1[:, 0] - p2[:,0]) + np.abs(p1[:,1] - p2[:,1])\n",
    "\n",
    "# Chebyshev distance between two point\n",
    "def chebyshev_distance(p1,p2):\n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "    return np.maximum(np.abs(p1[:, 0] - p2[:,0]), np.abs(p1[:,1] - p2[:,1]))\n",
    "\n",
    "# Ditance from the nearest window\n",
    "def min_window_distance(p):\n",
    "    window_coords = [[1., 0.], [3., 0.], [6., 0.], [8., 0.], [10., 0.],\n",
    "                     [1., 9.], [3., 9.], [6., 9.], [8., 9.], [10., 9.]]\n",
    "    if len(np.array(p).shape)==1:\n",
    "        p=np.array([p])\n",
    "    return np.min([euclidean_distance(p, np.array([w])) for w in window_coords], axis=0)\n",
    "\n",
    "# Center distance\n",
    "def center_distance(p):\n",
    "    center = [5.,5.]\n",
    "    if len(np.array(p).shape)==1:\n",
    "        p=np.array([p])\n",
    "    return euclidean_distance(np.array([center]), p)\n",
    "\n",
    "# Window distance between two point\n",
    "def window_dist_similarity(p1,p2):\n",
    "    window_coords = [[1., 0.], [3., 0.], [6., 0.], [8., 0.], [10., 0.],\n",
    "                     [1., 9.], [3., 9.], [6., 9.], [8., 9.], [10., 9.]]\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "\n",
    "    d1 = np.min([euclidean_distance(p1, np.array([w])) for w in window_coords], axis=0)\n",
    "    d2 = np.min([euclidean_distance(p2, np.array([w])) for w in window_coords], axis=0)\n",
    "    return np.abs(d1 - d2)\n",
    "\n",
    "# Center distance between two point\n",
    "def center_dist_similarity(p1,p2):\n",
    "    center = [5.,5.]\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])    \n",
    "    \n",
    "    d1 = euclidean_distance(np.array([center]), p1)\n",
    "    d2 = euclidean_distance(np.array([center]), p2)\n",
    "    return np.abs(d1 - d2)\n",
    "\n",
    "\n",
    "# Given all distances estimate the weighted distance \n",
    "def weighted_distance(p1, p2):    \n",
    "    ## dists=[ 'angular','euclidean', 'manhattan', 'c11hebyshev', 'window', 'center'] \n",
    "    \n",
    "    if len(np.array(p1).shape)==2:\n",
    "        p1 = np.array([[a,b] for (a,b) in p1])\n",
    "    if len(np.array(p2).shape)==2:\n",
    "        p2 = np.array([[a,b] for (a,b) in p2])\n",
    "    \n",
    "    dists=np.array([angular_distance(p1,p2), euclidean_distance(p1,p2), manhattan_distance(p1,p2),\n",
    "           chebyshev_distance(p1,p2), window_dist_similarity(p1,p2), center_dist_similarity(p1,p2)])\n",
    "    \n",
    "    rank_weights = [0.008478247652264459, 1.3935710976695117, 0.39760921865944, \n",
    "                    0.8009320148032265, 3.7056463223028255, -0.15417942722471556]\n",
    "    \n",
    "    res=np.dot(dists.T,rank_weights)\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def my_sqrt(arg1):\n",
    "    return np.nan_to_num(np.sqrt(arg1))\n",
    "def my_log(arg1):\n",
    "    return np.nan_to_num(np.log(arg1))\n",
    "def my_abs(arg1):\n",
    "    return np.nan_to_num(np.abs(arg1))\n",
    "def my_neg(arg1):\n",
    "    return np.nan_to_num(np.negative(arg1))\n",
    "def my_square(arg1):\n",
    "    return np.nan_to_num(np.square(arg1))\n",
    "def my_add(arg1, arg2):\n",
    "    return np.nan_to_num(np.add(arg1, arg2))\n",
    "def my_sub(arg1, arg2):\n",
    "    return np.nan_to_num(np.subtract(arg1, arg2))\n",
    "def my_mul(arg1, arg2):\n",
    "    return np.nan_to_num(np.multiply(arg1, arg2))\n",
    "def my_div(arg1, arg2):   \n",
    "    return np.nan_to_num(np.divide(arg1, arg2))\n",
    "def my_pow(arg1, arg2):\n",
    "    return np.nan_to_num(np.power(arg1, arg2))\n",
    "def my_max(arg1, arg2):\n",
    "    return np.nan_to_num(np.maximum(arg1, arg2), nan=sys.float_info.min)\n",
    "def my_min(arg1, arg2):\n",
    "    return np.nan_to_num(np.minimum(arg1, arg2), nan=sys.float_info.max)\n",
    "\n",
    "def gp_func(ARG0,ARG1,ARG2,ARG3):\n",
    "    return my_max(my_add(my_div(my_max(ARG3, my_log(ARG1)), my_sub(my_sqrt(0.28819287526174175), my_add(-0.8372387615502006, ARG1))), my_max(0.4761407317066777, ARG3)), my_max(-0.3152168742592161, my_div(0.8792291653581332, ARG0)))\n",
    "\n",
    "# Given all distances estimate the weighted distance \n",
    "def gp_distance(p1, p2):    \n",
    "\n",
    "    ## dists=[ 'angular','euclidean', 'manhattan', 'c11hebyshev', 'window', 'center'] \n",
    "    \n",
    "    if len(np.array(p1).shape)==2:\n",
    "        p1 = np.array([[a,b] for (a,b) in p1])\n",
    "    if len(np.array(p2).shape)==2:\n",
    "        p2 = np.array([[a,b] for (a,b) in p2])\n",
    "    \n",
    "    res = gp_func(angular_distance(p1,p2), euclidean_distance(p1,p2), manhattan_distance(p1,p2),\n",
    "           chebyshev_distance(p1,p2))\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe that collects all the results\n",
    "results = pd.DataFrame(columns=['method', 'fold', 'median_error', '95_perc_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM: Generating the dataset: TRAINING AND VALIDATION\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "dataframe_training = all_training_data[all_training_data['node'].isin(eval_sensors)]\n",
    "\n",
    "X_data = np.full((len(dataframe_training), 7), -111.)\n",
    "# Temporal information\n",
    "X_data[:, 0] = dataframe_training['moy_sin']\n",
    "# X_data[:, 1] = dataframe_training['moy_cos']\n",
    "X_data[:, 1] = dataframe_training['dow_sin']\n",
    "# X_data[:, 3] = dataframe_training['dow_cos']\n",
    "X_data[:, 2] = dataframe_training['seconds_from_midnight_sin']\n",
    "X_data[:, 3] = dataframe_training['seconds_from_midnight_cos']\n",
    "\n",
    "# # Distances from the reference sensors\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#         X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#         X_data[:, 4+i] = eucl_dist(X_coord_ref, Y_coord_ref, dataframe_training['coord_x'], dataframe_training['coord_y'])\n",
    "        \n",
    "# # Mutual center distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     center_distance = eucl_dist(dataframe_training['coord_x'], dataframe_training['coord_y'], 5, 5)\n",
    "#     ref_center_distance = eucl_dist(X_coord_ref, Y_coord_ref, 5, 5)\n",
    "#     center_distance_similarity = abs(ref_center_distance - center_distance)\n",
    "#     X_data[:, 9+i] = center_distance_similarity    \n",
    "    \n",
    "# Min window distance\n",
    "# X_data[:, 4] = np.min([eucl_dist(dataframe_training['coord_x'], dataframe_training['coord_y'], w_x, w_y) for (w_x, w_y) in window_coords], axis=0)   \n",
    "# # Center distance\n",
    "# X_data[:, 10] = eucl_dist(dataframe_training['coord_x'], dataframe_training['coord_y'], 5, 5)  \n",
    "\n",
    "X_data[:, 4] = dataframe_training['coord_x']\n",
    "X_data[:, 5] = dataframe_training['coord_y']\n",
    "\n",
    "# Weighted distance\n",
    "for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_data[:, 6+i] = gp_distance(ref_points,sens_points)\n",
    "\n",
    "\n",
    "assert np.min(X_data) > -111.\n",
    "\n",
    "\n",
    "# Now the temporal data\n",
    "history_timesteps = 17 # so, a total of 18 values, 17 historical and 1 current\n",
    "\n",
    "X_data_temporal_aux = np.full((training_data_ts, history_timesteps+1, 4), -111.)\n",
    "\n",
    "ref_temps = np.full((training_data_ts, 4), -1.)\n",
    "ref_temps[:, 0] = all_training_data[all_training_data['node'] == 'raspihat08']['temperature'].values\n",
    "ref_temps[:, 1] = all_training_data[all_training_data['node'] == 'raspihat09']['temperature'].values\n",
    "ref_temps[:, 2] = all_training_data[all_training_data['node'] == 'raspihat01']['temperature'].values\n",
    "ref_temps[:, 3] = all_training_data[all_training_data['node'] == 'raspihat04']['temperature'].values\n",
    "\n",
    "for row in range(training_data_ts):\n",
    "    for col_idx in range(4):\n",
    "        history = ref_temps[max(0,row-history_timesteps):row+1, col_idx]\n",
    "\n",
    "        history = np.pad(history, (history_timesteps+1-history.shape[0], 0), \n",
    "                         'constant', constant_values=(-1, -1))\n",
    "\n",
    "        X_data_temporal_aux[row, :, col_idx] =  history\n",
    "\n",
    "X_data_temporal = array_rep(X_data_temporal_aux, len(eval_sensors))\n",
    "\n",
    "X_data_temporal[X_data_temporal == -1.] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Now the y data\n",
    "\n",
    "y_data = dataframe_training['temperature'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting into training and eval data (week based splitting)\n",
    "np.random.seed(42)\n",
    "weeks_list = dataframe_training.year_week.unique().tolist()\n",
    "np.random.shuffle(weeks_list)\n",
    "train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc_training)])\n",
    "# val_weeks =  sorted(weeks_list[int(len(weeks_list)*perc_training):])\n",
    "\n",
    "\n",
    "\n",
    "# Different weeks between training and validation...\n",
    "X_data_training = X_data[dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "X_data_temporal_training = X_data_temporal[dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "y_data_training = y_data[dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "\n",
    "X_data_validation = X_data[~ dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "X_data_temporal_validation = X_data_temporal[~ dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "y_data_validation = y_data[~ dataframe_training['year_week'].isin(train_weeks)].copy()\n",
    "\n",
    "\n",
    "print(X_data_training.shape, X_data_validation.shape)\n",
    "print(X_data_temporal_training.shape, X_data_temporal_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_data_training=scaler.fit_transform(X_data_training)\n",
    "X_data_validation=scaler.transform(X_data_validation)\n",
    "\n",
    "X_data_temporal_training_min = np.nanmin(X_data_temporal_training, axis=(0, 1), keepdims=True)\n",
    "X_data_temporal_training_max = np.nanmax(X_data_temporal_training, axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_data_temporal_training = np.nan_to_num((X_data_temporal_training - X_data_temporal_training_min) / (X_data_temporal_training_max - X_data_temporal_training_min), nan=-1.)\n",
    "X_data_temporal_validation = np.nan_to_num((X_data_temporal_validation - X_data_temporal_training_min) / (X_data_temporal_training_max - X_data_temporal_training_min), nan=-1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/Abhiswain97/b316dfbc0fec644009335f0748ff6a8d#file-pytorch-tpu-efficientnet-b5-tutorial-reference-ipynb\n",
    "# https://www.kaggle.com/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r\n",
    "\n",
    "# TPU imports\n",
    "\n",
    "import warnings\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Other imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "# https://www.kaggle.com/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def calcola_scarto(len_dataset, batch_s, n_tpu):\n",
    "    return len_dataset - math.floor((len_dataset/(batch_s*n_tpu)))*(batch_s*n_tpu)\n",
    "\n",
    "prova_batch = list(range(100, 5000, 1))\n",
    "risultati = []\n",
    "for bs in prova_batch:\n",
    "    tr = calcola_scarto (len(y_data_training), bs, 8)\n",
    "    val = calcola_scarto (len(y_data_validation), bs, 8)\n",
    "    risultati.append((bs, tr, val, tr+val))\n",
    "    \n",
    "sorted(risultati, key=lambda tup: tup[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to determine how many TPUs to use, and the batch size, so to reduce the number of discarded data\n",
    "import math\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "\n",
    "def calcola_scarto(len_dataset, batch_s, n_tpu):\n",
    "    return len_dataset - math.floor((len_dataset/(batch_s*n_tpu)))*(batch_s*n_tpu)\n",
    "\n",
    "\n",
    "print(\"Istanze che verranno scartate nel training:\", calcola_scarto(len(y_data_training), BATCH_SIZE, NUM_TPU_CORES), \"in frazione:\", round(calcola_scarto(len(y_data_training), BATCH_SIZE, NUM_TPU_CORES)/len(y_data_training),2))\n",
    "print(\"Istanze che verranno scartate nel validation:\", calcola_scarto(len(y_data_validation), BATCH_SIZE, NUM_TPU_CORES), \"in frazione:\", round(calcola_scarto(len(y_data_validation), BATCH_SIZE, NUM_TPU_CORES)/len(y_data_validation),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From numpy arrays to tensor Datasets\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_data_temporal_training.astype('float32')), # temporal\n",
    "                                          torch.from_numpy(X_data_training.astype('float32')), # atemporal  \n",
    "                                          torch.from_numpy(y_data_training.astype('float32').reshape(-1, 1)))\n",
    "\n",
    "validationset = torch.utils.data.TensorDataset(torch.from_numpy(X_data_temporal_validation.astype('float32')),\n",
    "                                               torch.from_numpy(X_data_validation.astype('float32')), \n",
    "                                               torch.from_numpy(y_data_validation.astype('float32').reshape(-1, 1)))  \n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network: temporal + atemporal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from typing import *\n",
    "\n",
    "\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the same dropout mask across the temporal dimension\n",
    "    See https://arxiv.org/abs/1512.05287 for more details.\n",
    "    Note that this is not applied to the recurrent activations in the LSTM like the above paper.\n",
    "    Instead, it is applied to the inputs and outputs of the recurrent layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: float, batch_first: Optional[bool]=False):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.dropout <= 0.:\n",
    "            return x\n",
    "\n",
    "        is_packed = isinstance(x, PackedSequence)\n",
    "        if is_packed:\n",
    "            x, batch_sizes = x\n",
    "            max_batch_size = int(batch_size[0])\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = x.size(0)\n",
    "\n",
    "        # Drop same mask across entire sequence\n",
    "        if self.batch_first:\n",
    "            m = x.new_empty(max_batch_size, 1, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
    "        else:\n",
    "            m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
    "        x = x.masked_fill(m == 0, 0) / (1 - self.dropout)\n",
    "\n",
    "        if is_packed:\n",
    "            return PackedSequence(x, batch_sizes)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "\n",
    "\n",
    "class LSTM(nn.LSTM):\n",
    "    def __init__(self, *args, dropouti: float=0.,\n",
    "                 dropoutw: float=0., dropouto: float=0.,\n",
    "                 batch_first=True, unit_forget_bias=True, **kwargs):\n",
    "        super().__init__(*args, **kwargs, batch_first=batch_first)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "        self.dropoutw = dropoutw\n",
    "        self.input_drop = VariationalDropout(dropouti,\n",
    "                                             batch_first=batch_first)\n",
    "        self.output_drop = VariationalDropout(dropouto,\n",
    "                                              batch_first=batch_first)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Use orthogonal init for recurrent layers, xavier uniform for input layers\n",
    "        Bias is 0 except for forget gate\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif \"bias\" in name and self.unit_forget_bias:\n",
    "                nn.init.zeros_(param.data)\n",
    "                param.data[self.hidden_size:2 * self.hidden_size] = 1\n",
    "\n",
    "    def _drop_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_hh\" in name:\n",
    "                getattr(self, name).data = \\\n",
    "                    torch.nn.functional.dropout(param.data, p=self.dropoutw,\n",
    "                                                training=self.training).contiguous()\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        self._drop_weights()\n",
    "        input = self.input_drop(input)\n",
    "        seq, state = super().forward(input, hx=hx)\n",
    "        return self.output_drop(seq), state\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "      \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Temporal part\n",
    "        self.lstm_1= LSTM(input_size=4, hidden_size=128, num_layers=1, bidirectional=False, batch_first=True)#, dropoutw=0.1)#, dropouto=0.1, dropouti=0.1)\n",
    "        self.ln_1 = nn.LayerNorm(128)\n",
    "        \n",
    "        # Atemporal part\n",
    "        self.fc_1 = nn.Linear(7, 64) \n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Combined part\n",
    "        self.bn_comb_0 = nn.BatchNorm1d(192)\n",
    "        self.drop_comb_0 = nn.Dropout(0.1)  \n",
    "        self.fc_comb_1 = nn.Linear(192, 128) \n",
    "        self.bn_comb_1 = nn.BatchNorm1d(128)\n",
    "        self.fc_comb_2 = nn.Linear(128, 1) \n",
    "        \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        # Temporal part\n",
    "        x, states = self.lstm_1(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.ln_1(x)\n",
    "        \n",
    "        # Atemporal part\n",
    "        y = self.bn_1(F.relu(self.fc_1(y)))\n",
    "                \n",
    "        # Combined part\n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = self.drop_comb_0(self.bn_comb_0(z))\n",
    "        z = self.bn_comb_1(F.relu(self.fc_comb_1(z)))\n",
    "        z = self.fc_comb_2(z)\n",
    "            \n",
    "        return z\n",
    "\n",
    "    \n",
    "# Helper function that is used to (re)define the model and the optimizer from scratch\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "def reinit_model():\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    net = Net()\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "model = reinit_model()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some helper functions for the model that guide the training\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def reduce_fn_avg(vals):\n",
    "    # take average\n",
    "    return sum(vals) / len(vals)\n",
    "\n",
    "\n",
    "def reduce_fn_sum(vals):\n",
    "    # take average\n",
    "    return sum(vals)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _run(model, EPOCHS, param, training_data_in, validation_data_in=None):\n",
    "    \n",
    "    xm.set_rng_state(42)\n",
    "    xm.save(xm.get_rng_state(), 'xm_seed')    \n",
    "\n",
    "    def train_fn(train_dataloader, model, optimizer, criterion, device, lr_scheduler=None):\n",
    "        \n",
    "        xm.set_rng_state(torch.load('xm_seed'), device=device)\n",
    "        \n",
    "        \n",
    "        xm.master_print(xm.get_rng_state())\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_mae = 0.\n",
    "        running_mse = 0.\n",
    "        running_instances = 0.\n",
    "\n",
    "        # training() is a kind of switch for some specific layers/parts of the model that behave\n",
    "        # differently during training and inference (evaluating) time\n",
    "        # For example, Dropouts Layers, BatchNorm Layers etc. \n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (temporal, atemporal, labels) in enumerate(train_dataloader, 1):\n",
    "\n",
    "            optimizer.zero_grad() # need to zero out the gradients every time, otherwise they accumulate\n",
    "            temporal = temporal.to(device) # transfer the data to the computing device\n",
    "            atemporal = atemporal.to(device) # transfer the data to the computing device\n",
    "            labels = labels.to(device)# transfer the labels to the computing device     \n",
    "                \n",
    "            outputs = model(temporal, atemporal)\n",
    "                                    \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            #xm.master_print(f'Batch: {batch_idx}, loss: {loss.item()}')\n",
    "                        \n",
    "            loss.backward() # calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            xm.optimizer_step(optimizer) # update the network weights\n",
    "                                                \n",
    "            running_loss += loss.item()*len(labels)\n",
    "            running_instances += len(labels)\n",
    "            \n",
    "            predicted = outputs.data\n",
    "            \n",
    "            mse = torch.square(labels - predicted).sum().item() \n",
    "            mae = torch.abs(labels - predicted).sum().item() \n",
    "            \n",
    "            running_mse += mse\n",
    "            running_mae += mae\n",
    "\n",
    "            \n",
    "            if lr_scheduler != None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                \n",
    "        running_loss /= running_instances\n",
    "        loss_reduced = xm.mesh_reduce('loss_reduced', running_loss, reduce_fn_avg)\n",
    "        \n",
    "        running_mse /= running_instances\n",
    "        running_mae /= running_instances\n",
    "        mse_reduced = xm.mesh_reduce('mse_reduce', running_mse, reduce_fn_avg) \n",
    "        mae_reduced = xm.mesh_reduce('mae_reduce', running_mae, reduce_fn_avg) \n",
    "        \n",
    "        retval = {'loss':  loss_reduced,\n",
    "                  'mse': mse_reduced,\n",
    "                  'mae': mae_reduced\n",
    "                 }\n",
    "        \n",
    "        xm.save(xm.get_rng_state(), 'xm_seed')\n",
    "            \n",
    "        return retval\n",
    "            \n",
    "\n",
    "        \n",
    "    def valid_fn(valid_dataloader, model, criterion, device):\n",
    "            \n",
    "        xm.save(xm.get_rng_state(), 'xm_seed')\n",
    "            \n",
    "        running_loss = 0.\n",
    "        running_mae = 0.\n",
    "        running_mse = 0.\n",
    "        running_instances = 0.\n",
    "         \n",
    "        # eval() is a kind of switch for some specific layers/parts of the model that behave\n",
    "        # differently during training and inference (evaluating) time\n",
    "        # For example, Dropouts Layers, BatchNorm Layers etc. \n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, (temporal, atemporal, labels) in enumerate(valid_dataloader, 1):\n",
    "\n",
    "            temporal = temporal.to(device)\n",
    "            atemporal = atemporal.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(temporal, atemporal)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()*len(labels)\n",
    "            running_instances += len(labels)\n",
    "            \n",
    "            predicted = outputs.data\n",
    "            \n",
    "            mse = torch.square(labels - predicted).sum().item() \n",
    "            mae = torch.abs(labels - predicted).sum().item() \n",
    "            \n",
    "            mse_reduced = xm.mesh_reduce('mse_reduce', mse, reduce_fn_sum) \n",
    "            mae_reduced = xm.mesh_reduce('mae_reduce', mae, reduce_fn_sum) \n",
    "            \n",
    "            running_mse += mse\n",
    "            running_mae += mae\n",
    "            \n",
    "        \n",
    "        running_loss /= running_instances\n",
    "        loss_reduced = xm.mesh_reduce('loss_reduced', running_loss, reduce_fn_avg)    \n",
    "        \n",
    "        running_mse /= running_instances\n",
    "        running_mae /= running_instances\n",
    "        mse_reduced = xm.mesh_reduce('mse_reduce', running_mse, reduce_fn_avg) \n",
    "        mae_reduced = xm.mesh_reduce('mae_reduce', running_mae, reduce_fn_avg) \n",
    "        \n",
    "        retval = {'loss': loss_reduced, \n",
    "                  'mse': mse_reduced,\n",
    "                  'mae': mae_reduced\n",
    "                 }\n",
    "                    \n",
    "            \n",
    "        xm.set_rng_state(torch.load('xm_seed'), device=device)\n",
    "        xm.master_print(xm.get_rng_state())\n",
    "        \n",
    "        return retval\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Defining distributed samplers and data loaders\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(training_data_in,\n",
    "                                                                    num_replicas=xm.xrt_world_size(), #numcores\n",
    "                                                                    rank=xm.get_ordinal(),\n",
    "                                                                    shuffle=True)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data_in, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=1) # only for GPUs num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "    \n",
    "    if validation_data_in != None:\n",
    "        validation_sampler = torch.utils.data.distributed.DistributedSampler(validation_data_in,\n",
    "                                                                             num_replicas=xm.xrt_world_size(),\n",
    "                                                                             rank=xm.get_ordinal(),\n",
    "                                                                             shuffle=True)\n",
    "\n",
    "        validation_dataloader = torch.utils.data.DataLoader(validation_data_in, batch_size=BATCH_SIZE, sampler=validation_sampler, num_workers=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Defining the handle to the TPU device\n",
    "    device = xm.xla_device()\n",
    "    # Transferring the model to the computing device\n",
    "    model.to(device) \n",
    "    # Defining the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "        \n",
    "    # Defining optimizer\n",
    "    import torch.optim as optim\n",
    "    optimizer = optim.Adam(model.parameters(), lr=9e-5, amsgrad=False) #3e-4\n",
    "    lr_scheduler = None\n",
    "    \n",
    "    # Training code\n",
    "    \n",
    "    metrics_history = {\"loss\":[], \"mae\":[], \"mse\":[], \"val_loss\":[], \"val_mae\":[], \"val_mse\":[]}\n",
    "    \n",
    "    train_begin = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device], fixed_batch_size=True) # needed for parallel training\n",
    "\n",
    "        xm.master_print(\"EPOCH:\", epoch+1)\n",
    "\n",
    "        train_metrics = train_fn(train_dataloader=para_loader.per_device_loader(device), \n",
    "                                 model=model,\n",
    "                                 optimizer=optimizer, \n",
    "                                 criterion=criterion,\n",
    "                                 device=device,\n",
    "                                 lr_scheduler=lr_scheduler)\n",
    "        \n",
    "        metrics_history[\"loss\"].append(train_metrics[\"loss\"])\n",
    "        metrics_history[\"mae\"].append(train_metrics[\"mae\"])\n",
    "        metrics_history[\"mse\"].append(train_metrics[\"mse\"])\n",
    "        \n",
    "            \n",
    "        \n",
    "        if validation_data_in != None:    \n",
    "            # Calculate the metrics on the validation data, in the same way as done for training\n",
    "            with torch.no_grad(): # don't keep track of the info necessary to calculate the gradients\n",
    "                para_loader = pl.ParallelLoader(validation_dataloader, [device], fixed_batch_size=True)\n",
    "\n",
    "                val_metrics = valid_fn(valid_dataloader=para_loader.per_device_loader(device), \n",
    "                                       model=model,\n",
    "                                       criterion=criterion, \n",
    "                                       device=device)\n",
    "\n",
    "                metrics_history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "                metrics_history[\"val_mae\"].append(val_metrics[\"mae\"])\n",
    "                metrics_history[\"val_mse\"].append(val_metrics[\"mse\"])\n",
    "                \n",
    "            xm.master_print(\"  > Training/validation loss:\", round(train_metrics['loss'], 4), round(val_metrics['loss'], 4))\n",
    "            xm.master_print(\"  > Training/validation mae:\", round(train_metrics['mae'], 4), round(val_metrics['mae'], 4))\n",
    "            xm.master_print(\"  > Training/validation mse:\", round(train_metrics['mse'], 4), round(val_metrics['mse'], 4))\n",
    "        else:\n",
    "            xm.master_print(\"  > Training loss:\", round(train_metrics['loss'], 4))\n",
    "            xm.master_print(\"  > Training loss:\", round(train_metrics['mae'], 4))\n",
    "            xm.master_print(\"  > Training loss:\", round(train_metrics['mse'], 4))\n",
    "\n",
    "\n",
    "        xm.master_print(\"Completed in:\", round(time.time() - start, 1), \"seconds \\n\")\n",
    "\n",
    "    xm.master_print(\"Training completed in:\", round((time.time()- train_begin)/60, 1), \"minutes\")    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Save the model weights\n",
    "    xm.save(\n",
    "        model.state_dict(), './nnet_model_physio.pt'\n",
    "    )\n",
    "    \n",
    "    # Save the metrics history\n",
    "    xm.save(metrics_history, 'training_history')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Prediction function, it runs on the CPU\n",
    "def predict_(model, data_in): \n",
    "    import math\n",
    "    \n",
    "    data_dataloader = torch.utils.data.DataLoader(data_in, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "    \n",
    "    model.load_state_dict(torch.load('./nnet_model_physio.pt'))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad(): # don't keep track of the info necessary to calculate the gradients \n",
    "        pbar = tqdm(desc=\"Minibatches: \", total=math.ceil(len(data_in)/BATCH_SIZE))\n",
    "        \n",
    "        for batch_idx, (temporal, atemporal) in enumerate(data_dataloader, 1):\n",
    "            outputs = model(temporal, atemporal)\n",
    "            predicted = outputs.data\n",
    "            predictions.extend(predicted.numpy())\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "    \n",
    "    return np.asarray(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the actual training process, WITH the validation data\n",
    "import pickle\n",
    "\n",
    "def smooth_curve(points, factor=0.5):\n",
    "    smoothed_points = []    \n",
    "    for point in points:\n",
    "        if len(smoothed_points) > 0:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous*factor + point*(1-factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    \n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "tuning_params = ['dummy']\n",
    "\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "\n",
    "print(tuning_params)\n",
    "tuning_results = []\n",
    "\n",
    "for param in tuning_params:\n",
    "    \n",
    "    print(param)\n",
    "\n",
    "    model = reinit_model() # reinitialize the model and optimizer before training\n",
    "    # model = reinit_model(param) # reinitialize the model and optimizer before training\n",
    "    def _mp_fn(rank, flags):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        _run(model, EPOCHS, param, training_data_in=trainset, validation_data_in=validationset)\n",
    "        if rank == 0:\n",
    "            time.sleep(2)\n",
    "\n",
    "    FLAGS={}\n",
    "    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=NUM_TPU_CORES, start_method='fork')\n",
    "    training_history = torch.load('training_history')\n",
    "    \n",
    "    print(\"Parameter\", param)\n",
    "    print(\"Minimum loss (smoothed) training/validation:\", np.round(np.min(smooth_curve(training_history['loss'])),4), np.round(np.min(smooth_curve(training_history['val_loss'])),4), \"at indexes\", np.argmin(smooth_curve(training_history['loss'])), np.argmin(smooth_curve(training_history['val_loss'])))\n",
    "    print(\"Minimum MSE (smoothed) training/validation:\", np.round(np.max(smooth_curve(training_history['mse'])),4), np.round(np.max(smooth_curve(training_history['val_mse'])),4), \"at indexes\", np.argmax(smooth_curve(training_history['mse'])), np.argmax(smooth_curve(training_history['val_mse'])))\n",
    "    print(\"Minimum MAE (smoothed) training/validation:\", np.round(np.max(smooth_curve(training_history['mae'])),4), np.round(np.max(smooth_curve(training_history['val_mae'])),4), \"at indexes\", np.argmax(smooth_curve(training_history['mae'])), np.argmax(smooth_curve(training_history['val_mae'])))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    tuning_results.append([param, training_history, np.round(np.min(smooth_curve(training_history['val_loss'])),4), np.round(np.max(smooth_curve(training_history['val_mse'])),4), np.argmin(smooth_curve(training_history['val_mae']))])\n",
    "    \n",
    "    pickle_file = open('./tuning_results', 'wb')\n",
    "    pickle.dump(tuning_results, pickle_file)\n",
    "    pickle_file.close()\n",
    "    \n",
    "\n",
    "tuning_results = pd.DataFrame(tuning_results, columns=['params', 'training_history', 'val_loss', 'val_mse', 'val_mae'])\n",
    "\n",
    "pickle_file = open('./tuning_results', 'wb')\n",
    "pickle.dump(tuning_results, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some statistics regarding the training process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "start_epoch = 10\n",
    "\n",
    "\n",
    "def smooth_curve(points, factor=0.5):\n",
    "    smoothed_points = []    \n",
    "    for point in points:\n",
    "        if len(smoothed_points) > 0:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous*factor + point*(1-factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    \n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(training_history['loss'][start_epoch:])+1), smooth_curve(training_history['loss'][start_epoch:]), label=\"training\")\n",
    "plt.plot(range(1, len(training_history['val_loss'][start_epoch:])+1), smooth_curve(training_history['val_loss'][start_epoch:]), label=\"validation\")\n",
    "#plt.xticks(range(1, len(training_history['train_loss'])+1, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(training_history['mse'][start_epoch:])+1), smooth_curve(training_history['mse'][start_epoch:]), label=\"training\")\n",
    "plt.plot(range(1, len(training_history['val_mse'][start_epoch:])+1), smooth_curve(training_history['val_mse'][start_epoch:]), label=\"validation\")\n",
    "#plt.xticks(range(1, len(training_history['train_loss'])+1, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Mean Squared Error\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(training_history['mae'][start_epoch:])+1), smooth_curve(training_history['mae'][start_epoch:]), label=\"training\")\n",
    "plt.plot(range(1, len(training_history['val_mae'][start_epoch:])+1), smooth_curve(training_history['val_mae'][start_epoch:]), label=\"validation\")\n",
    "#plt.xticks(range(1, len(training_history['train_loss'])+1, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Mean Absolute Error\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum loss (smoothed) training/validation:\", np.round(np.min(smooth_curve(training_history['loss'])),4), np.round(np.min(smooth_curve(training_history['val_loss'])),4), \"at indexes\", np.argmin(smooth_curve(training_history['loss'])), np.argmin(smooth_curve(training_history['val_loss'])))\n",
    "print(\"Minimum f1 (smoothed) training/validation:\", np.round(np.min(smooth_curve(training_history['mae'])),4), np.round(np.min(smooth_curve(training_history['val_mae'])),4), \"at indexes\", np.argmin(smooth_curve(training_history['mae'])), np.argmin(smooth_curve(training_history['val_mae'])))\n",
    "print(\"Minimum accuracy (smoothed) training/validation:\", np.round(np.min(smooth_curve(training_history['mse'])),4), np.round(np.min(smooth_curve(training_history['val_mse'])),4), \"at indexes\", np.argmin(smooth_curve(training_history['mse'])), np.argmin(smooth_curve(training_history['val_mse'])))\n",
    "print(\"\\n\")\n",
    "print(\"Minimum loss (NON smoothed) training/validation:\", np.round(np.min(training_history['loss']),4), np.round(np.min(training_history['val_loss']),4), \"at indexes\", np.argmin(training_history['loss']), np.argmin(training_history['val_loss']))\n",
    "print(\"Minimum f1 (NON smoothed) training/validation:\", np.round(np.min(training_history['mae']),4), np.round(np.min(training_history['val_mae']),4), \"at indexes\", np.argmin(training_history['mae']), np.argmin(training_history['val_mae']))\n",
    "print(\"Minimum accuracy (NON smoothed) training/validation:\", np.round(np.min(training_history['mse']),4), np.round(np.min(training_history['val_mse']),4), \"at indexes\", np.argmin(training_history['mse']), np.argmin(training_history['val_mse']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM: final model and predictions\n",
    "\n",
    "import random as rn\n",
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "\n",
    "\n",
    "to_delete = [x for x in results['method'] if 'lstm' in x]\n",
    "results = results[np.logical_not(results['method'].isin(to_delete))]\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "\n",
    "# Data that is common for each fold (training)\n",
    "common_data_training = np.full((training_data_ts, 4), -111.)\n",
    "# Temporal information\n",
    "common_data_training[:, 0] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_sin']\n",
    "# common_data_training[:, 1] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_training[:, 1] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_sin']\n",
    "# common_data_training[:, 3] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_training[:, 2] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_training[:, 3] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "\n",
    "\n",
    "# Data that is common for each fold (test)\n",
    "common_data_test = np.full((test_data_ts, 4), -111.)\n",
    "# Temporal information\n",
    "common_data_test[:, 0] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_sin']\n",
    "# common_data_test[:, 1] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_test[:, 1] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_sin']\n",
    "# common_data_test[:, 3] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_test[:, 2] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_test[:, 3] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "\n",
    "    \n",
    "    \n",
    "fold_sensors = sorted(eval_sensors)\n",
    "fold_data_training = all_training_data[all_training_data['node'].isin(fold_sensors)]\n",
    "\n",
    "X_train_data_fold = np.full((training_data_ts*len(fold_sensors), 7), -111.)\n",
    "    \n",
    "# Copy the common data\n",
    "X_train_data_fold[:, 0:4] = array_rep(common_data_training, len(fold_sensors))\n",
    "\n",
    "# # Distances from the reference sensors\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     X_train_data_fold[:, 6+i] = eucl_dist(X_coord_ref, Y_coord_ref, fold_data_training['coord_x'], fold_data_training['coord_y'])\n",
    "    \n",
    "# # Mutual center distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     ref_center_distance = eucl_dist(X_coord_ref, Y_coord_ref, 5, 5)\n",
    "#     center_distance = eucl_dist(fold_data_training['coord_x'], fold_data_training['coord_y'], 5, 5)   \n",
    "#     center_distance_similarity = abs(ref_center_distance - center_distance)\n",
    "#     X_train_data_fold[:, 9+i] = center_distance_similarity\n",
    "    \n",
    "# Min window distance    \n",
    "# X_train_data_fold[:, 4] = np.min([eucl_dist(fold_data_training['coord_x'], fold_data_training['coord_y'], w_x, w_y) for (w_x, w_y) in window_coords], axis=0)   \n",
    "\n",
    "# X and Y coord\n",
    "X_train_data_fold[:, 4] = fold_data_training['coord_x']\n",
    "X_train_data_fold[:, 5] = fold_data_training['coord_y']\n",
    "\n",
    "# Weighted distance\n",
    "for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = fold_data_training['coord_x'].values, fold_data_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_train_data_fold[:, 6+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "\n",
    "assert np.min(X_train_data_fold) > -111.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now the temporal data (training)\n",
    "history_timesteps = 17 # so, a total of 18 values, 17 historical and 1 current\n",
    "\n",
    "X_data_temporal_aux = np.full((training_data_ts, history_timesteps+1, 4), -111.)\n",
    "\n",
    "ref_temps = np.full((training_data_ts, 4), -1.)\n",
    "ref_temps[:, 0] = all_training_data[all_training_data['node'] == 'raspihat08']['temperature'].values\n",
    "ref_temps[:, 1] = all_training_data[all_training_data['node'] == 'raspihat09']['temperature'].values\n",
    "ref_temps[:, 2] = all_training_data[all_training_data['node'] == 'raspihat01']['temperature'].values\n",
    "ref_temps[:, 3] = all_training_data[all_training_data['node'] == 'raspihat04']['temperature'].values\n",
    "\n",
    "for row in range(training_data_ts):\n",
    "    for col_idx in range(4):\n",
    "        history = ref_temps[max(0,row-history_timesteps):row+1, col_idx]\n",
    "\n",
    "        history = np.pad(history, (history_timesteps+1-history.shape[0], 0), \n",
    "                         'constant', constant_values=(-1, -1))\n",
    "\n",
    "        X_data_temporal_aux[row, :, col_idx] =  history\n",
    "\n",
    "X_data_temporal_training = array_rep(X_data_temporal_aux, len(eval_sensors))\n",
    "\n",
    "X_data_temporal_training[X_data_temporal_training == -1.] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Now the temporal data (test)\n",
    "history_timesteps = 17 # so, a total of 18 values, 17 historical and 1 current\n",
    "\n",
    "X_data_temporal_aux = np.full((test_data_ts, history_timesteps+1, 4), -111.)\n",
    "\n",
    "ref_temps = np.full((test_data_ts, 4), -1.)\n",
    "ref_temps[:, 0] = all_test_data[all_test_data['node'] == 'raspihat08']['temperature'].values\n",
    "ref_temps[:, 1] = all_test_data[all_test_data['node'] == 'raspihat09']['temperature'].values\n",
    "ref_temps[:, 2] = all_test_data[all_test_data['node'] == 'raspihat01']['temperature'].values\n",
    "ref_temps[:, 3] = all_test_data[all_test_data['node'] == 'raspihat04']['temperature'].values\n",
    "\n",
    "for row in range(test_data_ts):\n",
    "    for col_idx in range(4):\n",
    "        history = ref_temps[max(0,row-history_timesteps):row+1, col_idx]\n",
    "\n",
    "        history = np.pad(history, (history_timesteps+1-history.shape[0], 0), \n",
    "                         'constant', constant_values=(-1, -1))\n",
    "\n",
    "        X_data_temporal_aux[row, :, col_idx] =  history\n",
    "\n",
    "X_data_temporal_test = X_data_temporal_aux\n",
    "\n",
    "X_data_temporal_test[X_data_temporal_test == -1.] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_data_fold)\n",
    "X_train_data_fold=scaler.transform(X_train_data_fold)\n",
    "\n",
    "X_data_temporal_training_min = np.nanmin(X_data_temporal_training, axis=(0, 1), keepdims=True)\n",
    "X_data_temporal_training_max = np.nanmax(X_data_temporal_training, axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_data_temporal_training = np.nan_to_num((X_data_temporal_training - X_data_temporal_training_min) / (X_data_temporal_training_max - X_data_temporal_training_min), nan=-1.)\n",
    "X_data_temporal_test = np.nan_to_num((X_data_temporal_test - X_data_temporal_training_min) / (X_data_temporal_training_max - X_data_temporal_training_min), nan=-1.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_data_temporal_training.astype('float32')), # temporal\n",
    "                                          torch.from_numpy(X_train_data_fold.astype('float32')), # atemporal  \n",
    "                                          torch.from_numpy(fold_data_training['temperature'].values.astype('float32').reshape(-1, 1)))\n",
    "\n",
    "import random as rn\n",
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "\n",
    "EPOCHS = 90\n",
    "\n",
    "model = reinit_model() # reinitialize the model and optimizer before training\n",
    "\n",
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    _run(model, EPOCHS, 'dummy', training_data_in=trainset, validation_data_in=None)\n",
    "    if rank == 0:\n",
    "        time.sleep(2)\n",
    "    \n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=NUM_TPU_CORES, start_method='fork')\n",
    "\n",
    "\n",
    "\n",
    "# Now I evaluate on each sensor separately\n",
    "pbar = tqdm(total=len(eval_sensors), desc=\"FOLDS done\")\n",
    "for sens_index, sensor in enumerate(eval_sensors):\n",
    "    fold_data_test = all_test_data[all_test_data['node'] == sensor]\n",
    "    \n",
    "    X_test_data_fold = np.full((test_data_ts, 7), -111.)\n",
    "    \n",
    "    # Copy the common data\n",
    "    X_test_data_fold[:, 0:4] = common_data_test\n",
    "    \n",
    "#     # Distances from the reference sensors\n",
    "#     for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#         X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]        \n",
    "#         X_test_data_fold[:, 6+i] = eucl_dist(X_coord_ref, Y_coord_ref, fold_data_test['coord_x'], fold_data_test['coord_y'])\n",
    "    \n",
    "#     # Mutual center distances\n",
    "#     for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#         X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#         ref_center_distance = eucl_dist(X_coord_ref, Y_coord_ref, 5, 5)        \n",
    "#         center_distance = eucl_dist(fold_data_test['coord_x'], fold_data_test['coord_y'], 5, 5)\n",
    "#         center_distance_similarity = abs(ref_center_distance - center_distance)\n",
    "#         X_test_data_fold[:, 9+i] = center_distance_similarity \n",
    "  \n",
    "    # Min window distance\n",
    "#     X_test_data_fold[:, 4] = np.min([eucl_dist(fold_data_test['coord_x'], fold_data_test['coord_y'], w_x, w_y) for (w_x, w_y) in window_coords], axis=0)\n",
    "\n",
    "    # X and Y coord\n",
    "    X_test_data_fold[:, 4] = fold_data_test['coord_x']   \n",
    "    X_test_data_fold[:, 5] = fold_data_test['coord_y']   \n",
    "    \n",
    "    # Weighted distance\n",
    "    for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "        X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "        ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "        X_coord, Y_coord = fold_data_test['coord_x'].values, fold_data_test['coord_y'].values\n",
    "        sens_points = list(zip(X_coord,Y_coord))\n",
    "        X_test_data_fold[:, 6+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "    \n",
    "    assert np.min(X_test_data_fold) > -111.\n",
    "\n",
    "    # Normalizing predictors\n",
    "    X_test_data_fold=scaler.transform(X_test_data_fold)\n",
    "    \n",
    "    \n",
    "    # Predict the values, and evaluate the errors\n",
    "    \n",
    "    testset = torch.utils.data.TensorDataset(torch.from_numpy(X_data_temporal_test.astype('float32')), # temporal\n",
    "                                             torch.from_numpy(X_test_data_fold.astype('float32'))) # atemporal  \n",
    "    \n",
    "    fold_preds = predict_(model, testset).flatten()\n",
    "\n",
    "    \n",
    "    errors = abs(fold_data_test['temperature'].values - fold_preds)\n",
    "    \n",
    "    median_error = np.percentile(errors, 50)\n",
    "    p95_error = np.percentile(errors, 95)\n",
    "    results.loc[len(results)] = ['lstm_' + str(history_timesteps), sensor, median_error, p95_error]\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "    \n",
    "results.sort_values(by=['method', 'fold'], inplace=True)\n",
    "\n",
    "pickle_out = open('results_nnets.pickle',\"wb\")\n",
    "pickle.dump(results, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 50\n",
    "\n",
    "print(np.percentile(results['95_perc_error'], perc))\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle_file = open('results.pickle', 'rb')\n",
    "res_other = pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "print(np.percentile(res_other[res_other['method'] == 'xgboost']['95_perc_error'], perc))\n",
    "\n",
    "prova = pd.concat([results, res_other])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "res_list = []\n",
    "for method in np.unique(prova['method']):\n",
    "    res_list.append(prova[prova['method'] == method]['95_perc_error'].values)\n",
    "plt.boxplot(res_list)\n",
    "plt.title(\"95th percentile error per method\")\n",
    "plt.ylabel(\"Error (temperature)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.xticks(np.asarray(list(range(len(np.unique(prova['method']))))) + 1, np.unique(prova['method']), rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
