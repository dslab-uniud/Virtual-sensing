{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from scipy.integrate import simps\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataframe from disk, if it exists\n",
    "\n",
    "try:\n",
    "    pickle_out_extended_dataframe = open('extended_dataframe.pickle',\"rb\")\n",
    "    data = pickle.load(pickle_out_extended_dataframe)\n",
    "    pickle_out_extended_dataframe.close()\n",
    "except IOError:\n",
    "    print(\"No previous dataframe is available\")\n",
    "\n",
    "# computing the year-week feature\n",
    "data['year_week'] = data.datetime.dt.strftime('%Y-%U')\n",
    "\n",
    "# For each sensor, determine the indexes to partition its data into training and test\n",
    "# In doing that, keep into account the temporal relationships\n",
    "# Meaning, test data is in the future wrt training data\n",
    "\n",
    "perc_training = 0.8\n",
    "\n",
    "\n",
    "split_type = 'week_based_random' # temporal_wise, day_based_random, full_random\n",
    "\n",
    "    \n",
    "# Checking that every sensor has the same length\n",
    "counts_per_sensor = data[['node', 'datetime']].groupby('node').count()\n",
    "assert np.max(counts_per_sensor['datetime']) == np.min(counts_per_sensor['datetime'])\n",
    "sensor_data_length = counts_per_sensor['datetime'][0]    \n",
    "\n",
    "   \n",
    "# Maps that store the training and test indexes for each sensor\n",
    "sens_map_train_indexes = {}\n",
    "sens_map_test_indexes = {}\n",
    "\n",
    "for sens in list(counts_per_sensor.index):\n",
    "    sens_map_train_indexes[sens] = []\n",
    "    sens_map_test_indexes[sens] = []    \n",
    "    \n",
    "    \n",
    "    \n",
    "if split_type == 'temporal_wise':\n",
    "    # first 70% of each sensor data is traning, and next 30% is test\n",
    "\n",
    "    # Generate training and test indexes\n",
    "\n",
    "    # I have already sorted the values by node and datetime in the original dataframe\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i*sensor_data_length\n",
    "        last_train = base_sens + int(sensor_data_length*perc_training)\n",
    "        last_test = last_train + int(sensor_data_length*(1 - perc_training)) + 1\n",
    "        train_inds = list(range(base_sens, last_train))\n",
    "        test_inds = list(range(last_train, last_test))\n",
    "        sens_map_train_indexes[sens] = train_inds\n",
    "        sens_map_test_indexes[sens] = test_inds\n",
    "        \n",
    "    for sens in list(counts_per_sensor.index):\n",
    "        first_train = sens_map_train_indexes[sens][0]\n",
    "        last_train = sens_map_train_indexes[sens][-1]\n",
    "        first_test = sens_map_test_indexes[sens][0]\n",
    "        last_test = sens_map_test_indexes[sens][-1]\n",
    "        assert first_train == data.query(\"node == @sens\").reset_index().iloc[0,0]\n",
    "        assert last_test == data.query(\"node == @sens\").reset_index().iloc[-1,0]\n",
    "        assert last_train > first_train and last_train < first_test\n",
    "        assert first_test > last_train and first_test < last_test\n",
    "    \n",
    "elif split_type == 'full_random':\n",
    "    # divide randomly the data of the sensors\n",
    "    \n",
    "    possible_indexes = list(range(sensor_data_length))\n",
    "    random.seed(42)\n",
    "    # I make sure to use the same split for each sensor\n",
    "    # meaning, corresponding training and test indexes of the different sensors refer to the same datetimes\n",
    "    train_indexes_overall = sorted(random.sample(possible_indexes, int(sensor_data_length * perc_training)))\n",
    "    test_indexes_overall = sorted(list(set(possible_indexes) - set(train_indexes_overall)))\n",
    "    \n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i * sensor_data_length\n",
    "        # it is sufficent to add the baseline, since all sensor tracks have the same length\n",
    "        sens_map_train_indexes[sens] = np.asarray(train_indexes_overall) + base_sens\n",
    "        sens_map_test_indexes[sens] = np.asarray(test_indexes_overall) + base_sens\n",
    "\n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(possible_indexes)\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'day_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    days_list = data.only_date.unique().tolist()\n",
    "    np.random.shuffle(days_list)\n",
    "    random_train_days = sorted(days_list[0:int(len(days_list)*perc_training)])\n",
    "    random_test_days =  sorted(days_list[int(len(days_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.only_date.isin(random_train_days)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.only_date.isin(random_test_days)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'week_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    weeks_list = data.year_week.unique().tolist()\n",
    "    np.random.shuffle(weeks_list)\n",
    "    random_train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc_training)])\n",
    "    random_test_weeks =  sorted(weeks_list[int(len(weeks_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.year_week.isin(random_train_weeks)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.year_week.isin(random_test_weeks)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "else:\n",
    "    assert False, 'Unsupported split type'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general information\n",
    "\n",
    "window_coords = [(1, 0), (3, 0), (6, 0), (8, 0), (10, 0),\n",
    "                 (1, 9), (3, 9), (6, 9), (8, 9), (10, 9)]\n",
    "\n",
    "all_train_idxs = []\n",
    "for sens in sens_map_train_indexes:\n",
    "    all_train_idxs.extend(sens_map_train_indexes[sens])\n",
    "\n",
    "all_test_idxs = []\n",
    "for sens in sens_map_test_indexes:\n",
    "    all_test_idxs.extend(sens_map_test_indexes[sens])\n",
    "\n",
    "node_map_coords = {}\n",
    "for sens in sens_map_test_indexes:\n",
    "    coord_x, coord_y = data[data['node'] == sens].iloc[0][['coord_x', 'coord_y']].values\n",
    "    node_map_coords[sens] = (coord_x, coord_y)\n",
    "    \n",
    "all_sensors = np.unique(data['node'])\n",
    "\n",
    "all_training_data = data.iloc[all_train_idxs].reset_index(drop=True)\n",
    "all_test_data = data.iloc[all_test_idxs].reset_index(drop=True)\n",
    "\n",
    "def eucl_dist(x_0, y_0, x_1, y_1):\n",
    "    return np.sqrt((x_0 - x_1)**2 + (y_0 - y_1)**2)\n",
    "\n",
    "training_data_ts = len(all_training_data[all_training_data['node'] == 'raspihat01'])\n",
    "\n",
    "test_data_ts = len(all_test_data[all_test_data['node'] == 'raspihat01'])\n",
    "\n",
    "air_tube_coords = [(x, 4) for x in list(range(11))]\n",
    "\n",
    "def array_rep(arr, n):\n",
    "    assert len(arr.shape) <= 2\n",
    "    if len(arr.shape) < 2:\n",
    "        return np.tile(arr, n)\n",
    "    else:\n",
    "        retarr = np.full((arr.shape[0]*n, arr.shape[1]), -1.)\n",
    "        for i in range(n):\n",
    "            retarr[i*arr.shape[0]: (i+1)*arr.shape[0]] = arr\n",
    "        return retarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe that collects all the results\n",
    "\n",
    "results = pd.DataFrame(columns=['method', 'fold', 'median_error', '95_perc_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Weighted distance utils functions\n",
    "\n",
    "## Spatial distances \n",
    "center = (5,5)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## Spatial distances \n",
    "\n",
    "# Angular distance between two point\n",
    "def angular_distance(p1,p2):\n",
    "    # determines the angle between two points (x_0, y_0) (x_1, y_1)\n",
    "    # if p2 is the origin (0, 0), then I get the angle wrt x axis\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "\n",
    "    ang1 = np.arctan2(p1[:,1], p1[:,0])\n",
    "    ang2 = np.arctan2(p2[:,1], p2[:,0])\n",
    "    \n",
    "    return np.minimum(np.rad2deg(np.mod(ang1 - ang2, 2 * np.pi)), np.rad2deg(np.mod(ang1 - ang2, 2 * np.pi)))\n",
    "    \n",
    "# Euclidean distance between two point\n",
    "def euclidean_distance(p1,p2):    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "    return np.sqrt(np.add(np.subtract(p1[:,0],p2[:,0])**2, np.subtract(p1[:,1],p2[:,1])**2))\n",
    "\n",
    "# Manhattan distance between two point\n",
    "def manhattan_distance(p1,p2):\n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])    \n",
    "    return np.abs(p1[:, 0] - p2[:,0]) + np.abs(p1[:,1] - p2[:,1])\n",
    "\n",
    "# Chebyshev distance between two point\n",
    "def chebyshev_distance(p1,p2):\n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "    return np.maximum(np.abs(p1[:, 0] - p2[:,0]), np.abs(p1[:,1] - p2[:,1]))\n",
    "\n",
    "# Ditance from the nearest window\n",
    "def min_window_distance(p):\n",
    "    window_coords = [[1., 0.], [3., 0.], [6., 0.], [8., 0.], [10., 0.],\n",
    "                     [1., 9.], [3., 9.], [6., 9.], [8., 9.], [10., 9.]]\n",
    "    if len(np.array(p).shape)==1:\n",
    "        p=np.array([p])\n",
    "    return np.min([euclidean_distance(p, np.array([w])) for w in window_coords], axis=0)\n",
    "\n",
    "# Center distance\n",
    "def center_distance(p):\n",
    "    center = [5.,5.]\n",
    "    if len(np.array(p).shape)==1:\n",
    "        p=np.array([p])\n",
    "    return euclidean_distance(np.array([center]), p)\n",
    "\n",
    "# Window distance between two point\n",
    "def window_dist_similarity(p1,p2):\n",
    "    window_coords = [[1., 0.], [3., 0.], [6., 0.], [8., 0.], [10., 0.],\n",
    "                     [1., 9.], [3., 9.], [6., 9.], [8., 9.], [10., 9.]]\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])\n",
    "\n",
    "    d1 = np.min([euclidean_distance(p1, np.array([w])) for w in window_coords], axis=0)\n",
    "    d2 = np.min([euclidean_distance(p2, np.array([w])) for w in window_coords], axis=0)\n",
    "    return np.abs(d1 - d2)\n",
    "\n",
    "# Center distance between two point\n",
    "def center_dist_similarity(p1,p2):\n",
    "    center = [5.,5.]\n",
    "    \n",
    "    if len(np.array(p1).shape)==1:\n",
    "        p1=np.array([p1])\n",
    "    if len(np.array(p2).shape)==1:\n",
    "        p2=np.array([p2])    \n",
    "    \n",
    "    d1 = euclidean_distance(np.array([center]), p1)\n",
    "    d2 = euclidean_distance(np.array([center]), p2)\n",
    "    return np.abs(d1 - d2)\n",
    "\n",
    "\n",
    "# Given all distances estimate the weighted distance \n",
    "def weighted_distance(p1, p2):    \n",
    "    ## dists=[ 'angular','euclidean', 'manhattan', 'c11hebyshev', 'window', 'center'] \n",
    "    \n",
    "    if len(np.array(p1).shape)==2:\n",
    "        p1 = np.array([[a,b] for (a,b) in p1])\n",
    "    if len(np.array(p2).shape)==2:\n",
    "        p2 = np.array([[a,b] for (a,b) in p2])\n",
    "    \n",
    "    dists=np.array([angular_distance(p1,p2), euclidean_distance(p1,p2), manhattan_distance(p1,p2),\n",
    "           chebyshev_distance(p1,p2), window_dist_similarity(p1,p2), center_dist_similarity(p1,p2)])\n",
    "    \n",
    "    rank_weights = [0.008478247652264459, 1.3935710976695117, 0.39760921865944, \n",
    "                    0.8009320148032265, 3.7056463223028255, -0.15417942722471556]\n",
    "    \n",
    "    res=np.dot(dists.T,rank_weights)\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def my_sqrt(arg1):\n",
    "    return np.nan_to_num(np.sqrt(arg1))\n",
    "def my_log(arg1):\n",
    "    return np.nan_to_num(np.log(arg1))\n",
    "def my_abs(arg1):\n",
    "    return np.nan_to_num(np.abs(arg1))\n",
    "def my_neg(arg1):\n",
    "    return np.nan_to_num(np.negative(arg1))\n",
    "def my_square(arg1):\n",
    "    return np.nan_to_num(np.square(arg1))\n",
    "def my_add(arg1, arg2):\n",
    "    return np.nan_to_num(np.add(arg1, arg2))\n",
    "def my_sub(arg1, arg2):\n",
    "    return np.nan_to_num(np.subtract(arg1, arg2))\n",
    "def my_mul(arg1, arg2):\n",
    "    return np.nan_to_num(np.multiply(arg1, arg2))\n",
    "def my_div(arg1, arg2):   \n",
    "    return np.nan_to_num(np.divide(arg1, arg2))\n",
    "def my_pow(arg1, arg2):\n",
    "    return np.nan_to_num(np.power(arg1, arg2))\n",
    "def my_max(arg1, arg2):\n",
    "    return np.nan_to_num(np.maximum(arg1, arg2), nan=sys.float_info.min)\n",
    "def my_min(arg1, arg2):\n",
    "    return np.nan_to_num(np.minimum(arg1, arg2), nan=sys.float_info.max)\n",
    "\n",
    "def gp_func(ARG0,ARG1,ARG2,ARG3):\n",
    "    return my_max(my_add(my_div(my_max(ARG3, my_log(ARG1)), my_sub(my_sqrt(0.28819287526174175), my_add(-0.8372387615502006, ARG1))), my_max(0.4761407317066777, ARG3)), my_max(-0.3152168742592161, my_div(0.8792291653581332, ARG0)))\n",
    "\n",
    "# Given all distances estimate the weighted distance \n",
    "def gp_distance(p1, p2):    \n",
    "\n",
    "    ## dists=[ 'angular','euclidean', 'manhattan', 'c11hebyshev', 'window', 'center'] \n",
    "    \n",
    "    if len(np.array(p1).shape)==2:\n",
    "        p1 = np.array([[a,b] for (a,b) in p1])\n",
    "    if len(np.array(p2).shape)==2:\n",
    "        p2 = np.array([[a,b] for (a,b) in p2])\n",
    "    \n",
    "    res = gp_func(angular_distance(p1,p2), euclidean_distance(p1,p2), manhattan_distance(p1,p2),\n",
    "           chebyshev_distance(p1,p2))\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline methods: IDW and classical average\n",
    "# Results are evaluated using leave-one-out cross-validation over 9 sensors, considering the test set\n",
    "# Also, I consider K-NN when evaluating those two methods (from 1 to 11)\n",
    "# Thus, evaluation is on 9 sensors (12 - the 3 refs sensors to be omogeneous wrt machine learning methods evaluation), while the knn can span all sensors (also the 3 ref sensors)\n",
    "\n",
    "ref_sensors_here = ['raspihat01','raspihat04','raspihat08','raspihat09']\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "\n",
    "to_delete = [x for x in results['method'] if 'average' in x]\n",
    "results = results[np.logical_not(results['method'].isin(to_delete))]\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# for each value of K-NN\n",
    "pbar = tqdm(total=len(all_sensors)-1, desc=\"KNNS done\")\n",
    "for knns in range(1, len(all_sensors)):\n",
    "    # for each evaluation fold\n",
    "    for sensor in eval_sensors:\n",
    "        # Getting landmark sensors\n",
    "        landmark_sensors = sorted(list(set(all_sensors) - set([sensor])))\n",
    "        # Getting info on the sensor to predict\n",
    "        X_coord, Y_coord = node_map_coords[sensor]\n",
    "        ground_truth = all_test_data[all_test_data['node'] == sensor]['temperature'].values\n",
    "        # Getting the K closest sensors\n",
    "        distances = []\n",
    "        for other_sensor in landmark_sensors:\n",
    "            other_X_coord, other_Y_coord = node_map_coords[other_sensor]\n",
    "            distances.append(euclidean_distance((X_coord, Y_coord), (other_X_coord, other_Y_coord))[0])\n",
    "        sorted_by_dist = [x for _,x in sorted(zip(distances, landmark_sensors))]\n",
    "        knn_sensors = sorted_by_dist[:knns]\n",
    "        knn_idws = 1 / np.asarray(sorted(distances)[:knns]) \n",
    "        # Generating the predictor columns\n",
    "        predictors = np.full((test_data_ts, len(knn_sensors)), -1.)\n",
    "        for col_idx, other_sensor in enumerate(knn_sensors):\n",
    "            predictors[:, col_idx] = all_test_data[all_test_data['node'] == other_sensor]['temperature'].values\n",
    "        \n",
    "        # Serve per plottare in ordine sul boxplot\n",
    "        stradd = \"\"\n",
    "        if knns > 9:\n",
    "            stradd = \"x\"\n",
    "        \n",
    "        # Performing the predictions\n",
    "        classic_avg_preds = np.mean(predictors, axis=1)\n",
    "        classic_avg_errors = np.abs(ground_truth - classic_avg_preds)\n",
    "        classic_avg_median_error = np.percentile(classic_avg_errors, 50)\n",
    "        classic_avg_95_error = np.percentile(classic_avg_errors, 95)\n",
    "        results.loc[len(results)] = ['classical_average_KNN-' + stradd + str(knns), sensor, classic_avg_median_error, classic_avg_95_error]\n",
    "        \n",
    "        \n",
    "        idw_avg_preds = np.average(predictors, axis=1, weights=knn_idws)\n",
    "        idw_avg_errors = np.abs(ground_truth - idw_avg_preds)\n",
    "        idw_avg_median_error = np.percentile(idw_avg_errors, 50)\n",
    "        idw_avg_95_error = np.percentile(idw_avg_errors, 95)\n",
    "        results.loc[len(results)] = ['IDW_average_KNN-' + stradd + str(knns), sensor, idw_avg_median_error, idw_avg_95_error]\n",
    " \n",
    "    pbar.update(1)   \n",
    "pbar.close()\n",
    "\n",
    "results.sort_values(by=['method', 'fold'], inplace=True)\n",
    "\n",
    "pickle_out = open('results.pickle',\"wb\")\n",
    "pickle.dump(results, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    \n",
    "    pickle_out = open('results.pickle',\"rb\")\n",
    "    results = pickle.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "#     skip_methods=['classical', 'IDW', 'particle', 'no_sensor7', 'whole_trainset']\n",
    "#     skip_methods = ['average','whole_trainset', 'no_sensor7', 'particle']\n",
    "    skip_methods = []\n",
    "\n",
    "    sel_methods = [x for x in np.unique(results['method']) if not any(y in x for y in skip_methods)]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    res_list = []\n",
    "    for method in sel_methods:\n",
    "        res_list.append(results[results['method'] == method]['median_error'].values)\n",
    "    plt.boxplot(res_list)\n",
    "    plt.title(\"Median error per method\")\n",
    "    plt.ylabel(\"Error (temperature)\")\n",
    "    plt.xlabel(\"Method\")\n",
    "    plt.xticks(np.asarray(list(range(len(sel_methods)))) + 1, sel_methods, rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    res_list = []\n",
    "    for method in sel_methods:\n",
    "        res_list.append(results[results['method'] == method]['95_perc_error'].values)\n",
    "    plt.boxplot(res_list)\n",
    "    plt.title(\"95th percentile error per method\")\n",
    "    plt.ylabel(\"Error (temperature)\")\n",
    "    plt.xlabel(\"Method\")\n",
    "    plt.xticks(np.asarray(list(range(len(sel_methods)))) + 1, sel_methods, rotation=90)\n",
    "               \n",
    "    plt.show()\n",
    "\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Particle filter implementation\n",
    "\n",
    "\n",
    "# Euclidean distance between two point\n",
    "def euclidean_distance_pf(p1,p2):\n",
    "    return math.sqrt(sum([(a - b) ** 2 for a, b in zip(p1, p2)]))\n",
    "\n",
    "# Window distance between two point\n",
    "def window_distance_similarity_pf(p1,p2): \n",
    "    d1 = min([euclidean_distance_pf(p1, w) for w in window_coords])\n",
    "    d2 = min([euclidean_distance_pf(p2, w) for w in window_coords])\n",
    "    return abs(d1 - d2)\n",
    "\n",
    "\n",
    "def create_uniform_particles(t_range, N):\n",
    "    particles = np.random.uniform(t_range[0], t_range[1], N)\n",
    "    return particles\n",
    "\n",
    "def get_nearest_landmark(pos, landmarks_pos, landmarks_temp):\n",
    "    index_min = np.argmin([euclidean_distance(pos, x) for x in landmarks_pos] )\n",
    "    # print('minindex', index_min, 'list', [ euclidean_distance(pos, x) for x in landmarks_pos]) #debug\n",
    "    return  landmarks_pos[index_min], landmarks_temp[index_min]\n",
    "    \n",
    "\n",
    "# predizione transizione particelle\n",
    "def predict(particles, u, std): # particles, u = delta ref temp, std = std.dev ref temp    \n",
    "    particles += u\n",
    "    particles += np.random.uniform(-1, 1, len(particles)) * std\n",
    "    \n",
    "def update_pos(particles, weights, zs, sd, landmarks, pos, landmarks_pos):\n",
    "    weights.fill(1.)\n",
    "    for i, landmark in enumerate(landmarks):\n",
    "        landmark_pos = landmarks_pos[i]\n",
    "        \n",
    "        distance = 0.5 * np.abs(particles - landmark)\n",
    "#         distance += 0.5 * weighted_distance(landmark_pos, pos)\n",
    "        distance += 0.3 * euclidean_distance_pf(landmark_pos, pos)\n",
    "        distance += 0.2 * window_distance_similarity_pf(landmark_pos, pos)\n",
    "        weights *= norm(distance, sd).pdf(zs[i])\n",
    "    weights += 1.e-300 # for numerical stability\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "# def neff(weights):\n",
    "#     return 1. / np.sum(np.square(weights))\n",
    "\n",
    "def systematic_resample(weights):\n",
    "    N = len(weights)\n",
    "    positions = (np.arange(N) + np.random.randn(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i') # 'i' -> integer\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N and j<N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "def estimate(particles, weights):\n",
    "#     var = np.average((particles - mean)**2, weights=weights, axis=0)\n",
    "    return np.average(particles, weights=weights, axis=0)\n",
    "\n",
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles = particles[indexes]\n",
    "    weights = weights[indexes]\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "def run_iteration_pos(particles, weights, ref_temp, previous_ref_temp, pos, landmarks, landmarks_pos, references_pos, sensor_std_err):\n",
    "    u=previous_ref_temp-ref_temp # u = delta ref temp\n",
    "    \n",
    "    # predict particles values based on ref changes\n",
    "    predict(particles, u, sensor_std_err) #t_std\n",
    "    \n",
    "    # predicted landmarks variantions wrt temp\n",
    "    zs = 0.5 * np.abs(landmarks - ref_temp) \n",
    "#     zs += 0.5 * np.array([weighted_distance(landmark_pos, references_pos) for landmark_pos in landmarks_pos]) \n",
    "    \n",
    "    zs += 0.3 * np.array([euclidean_distance_pf(landmark_pos, references_pos) for landmark_pos in landmarks_pos])\n",
    "    zs += 0.2 * np.array([window_distance_similarity_pf(landmark_pos, references_pos) for landmark_pos in landmarks_pos]) \n",
    "#     zs = zs + (np.random.randn(NL) * (sensor_std_err))\n",
    "#     print('update_zs time:', time.time() - elapsed_time)\n",
    "#     elapsed_time = time.time()\n",
    "    \n",
    "    update_pos(particles, weights, zs=zs, sd=sensor_std_err*0.5, landmarks=landmarks, pos=pos, landmarks_pos=landmarks_pos)\n",
    "#     print('update_particle time:', time.time() - elapsed_time)\n",
    "#     elapsed_time = time.time()\n",
    "\n",
    "    val=estimate(particles, weights) # weighted mean\n",
    "\n",
    "    indexes = systematic_resample(weights)\n",
    "    resample_from_index(particles, weights, indexes)\n",
    "    \n",
    "    ### estimate temp value from particles\n",
    "    return val\n",
    "\n",
    "def predict_cell_temps(x, y, landmarks, landmarks_pos, t_min, t_max):\n",
    "    global pbar\n",
    "    cell_temps = np.full(len(landmarks), -1.0)\n",
    "    \n",
    "    _, previous_ref_temp = get_nearest_landmark((x,y), landmarks_pos, landmarks[(6*60*24)-1,:])\n",
    "    \n",
    "    for i in range(len(landmarks)):\n",
    "        if i % (6*60*24) == 0: # reset particles each new day\n",
    "            particles=create_uniform_particles([t_min,t_max], N)\n",
    "            weights = np.full(N, 1.)\n",
    "        \n",
    "        \n",
    "        std = np.std(landmarks[i]) if len(landmarks[i])> 1 else 0.001\n",
    "        \n",
    "        reference_pos, reference_temp = get_nearest_landmark((x,y), landmarks_pos, landmarks[i,:])\n",
    "\n",
    "        cell_temps[i] = run_iteration_pos(particles, weights, reference_temp, previous_ref_temp, (x,y),\n",
    "                            landmarks[i,:], landmarks_pos, reference_pos, std)\n",
    "#                             test_data.iloc[i]['reference_temperatures_stddev']\n",
    "#         print(x , y, 'ref_pos', reference_pos, 'ref temp', reference_temp, 'estimated temp', cell_temps[i])\n",
    "\n",
    "        previous_ref_temp = reference_temp\n",
    "    \n",
    "    pbar.update(1)\n",
    "    \n",
    "    return cell_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [x for x in results['method'] if 'particle_filter' in x]\n",
    "results = results[np.logical_not(results['method'].isin(to_delete))]\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "\n",
    "start_knns=4\n",
    "\n",
    "# for each value of K-NN\n",
    "pbar = tqdm(total=(len(all_sensors)-start_knns)*len(eval_sensors), desc=\"KNNS done\")\n",
    "calls = np.full((len(all_sensors)-start_knns, len(eval_sensors) ), None)\n",
    "N=400 # n. paticles\n",
    "pool = Pool(processes=31)\n",
    "\n",
    "for knns in range(start_knns, len(all_sensors)):\n",
    "    # for each evaluation fold\n",
    "    for j,sensor in enumerate(eval_sensors):\n",
    "        # Getting landmark sensors\n",
    "        landmark_sensors = sorted(list(set(eval_sensors) - set([sensor])))\n",
    "        # Getting info on the sensor to predict\n",
    "        X_coord, Y_coord = node_map_coords[sensor]\n",
    "        \n",
    "        # Getting the K closest sensors\n",
    "        distances = []\n",
    "        for other_sensor in landmark_sensors:\n",
    "            other_X_coord, other_Y_coord = node_map_coords[other_sensor]\n",
    "            distances.append(euclidean_distance([X_coord, Y_coord], [other_X_coord, other_Y_coord]))\n",
    "        sorted_by_dist = [x for _,x in sorted(zip(distances, landmark_sensors))]\n",
    "        knn_sensors = sorted_by_dist[:knns]\n",
    "        \n",
    "        landmarks_pos = []\n",
    "        landmarks_temp = np.full((test_data_ts, len(knn_sensors)), -1.)\n",
    "        for i,os in enumerate(knn_sensors):\n",
    "            landmarks_pos.append(list(node_map_coords[os]))\n",
    "            landmarks_temp[:, i] = all_test_data.query(\"node==@os\")['temperature'].values\n",
    "        \n",
    "        landmarks_pos=np.array(landmarks_pos) \n",
    "        \n",
    "        t_min = all_training_data[all_training_data.node.isin(knn_sensors)]['temperature'].min()\n",
    "        t_max = all_training_data[all_training_data.node.isin(knn_sensors)]['temperature'].max()\n",
    "#         print(t_min, t_max)\n",
    "        calls[knns-start_knns,j] = pool.apply_async(predict_cell_temps, (X_coord, Y_coord, landmarks_temp.copy(), landmarks_pos.copy(), t_min, t_max))\n",
    "                                 \n",
    "        \n",
    "## read the results of the async calls\n",
    "\n",
    "# for each value of K-NN\n",
    "\n",
    "for knns in range(start_knns, len(all_sensors)):\n",
    "    for j,sensor in enumerate(eval_sensors):\n",
    "        ground_truth = all_test_data[all_test_data['node'] == sensor]['temperature'].values\n",
    "        predictors = calls[knns-start_knns, j].get()\n",
    "        \n",
    "        stradd = \"\"\n",
    "        if knns > 9:\n",
    "            stradd = \"x\"\n",
    "        \n",
    "        particle_errors = np.abs(ground_truth - predictors)\n",
    "        particle_median_error = np.percentile(particle_errors, 50)\n",
    "        particle_95_error = np.percentile(particle_errors, 95)\n",
    "        results.loc[len(results)] = ['particle_filter_KNN-' + stradd + str(knns), sensor, particle_median_error, particle_95_error]\n",
    "                \n",
    "pbar.close()\n",
    "\n",
    "results.sort_values(by=['method', 'fold'], inplace=True)\n",
    "\n",
    "pickle_out = open('results.pickle',\"wb\")\n",
    "pickle.dump(results, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature selection: \n",
    "# - the first step is removing highly correlated columns\n",
    "# - then, on the remainder, we run XGBoost\n",
    "#\n",
    "# Considered features\n",
    "# - MOY sin and cos\n",
    "# - DOW sin and cos\n",
    "# - SECS sin and cos\n",
    "# - 3 ref temperatures\n",
    "# - 3 distances from the reference sensors\n",
    "# - 3 mutual window distances\n",
    "# - 3 mutual center distances\n",
    "# - 3 mutual air tube distances\n",
    "# - min window distance\n",
    "# - min center distance\n",
    "# - min air tube distance\n",
    "# - X and Y coords of the cell to predict\n",
    "\n",
    "ref_sensors_here = ['raspihat01','raspihat04','raspihat08','raspihat09']\n",
    "\n",
    "features = ['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', \n",
    "            'seconds_from_midnight_sin', 'seconds_from_midnight_cos',\n",
    "            '01_ref_temp', '04_ref_temp', '08_ref_temp','09_ref_temp',\n",
    "            'X_coord', 'Y_coord',\n",
    "#             '01_ref_angulardist', '08_ref_angulardist','02_ref_angulardist', '04_ref_angulardist',\n",
    "#             '01_ref_euclideandist', '08_ref_euclideandist','02_ref_euclideandist', '04_ref_euclideandist',\n",
    "#             '01_ref_manhattandist', '08_ref_manhattandist','02_ref_manhattandist', '04_ref_manhattandist',\n",
    "#             '01_ref_chebyshevdist', '08_ref_chebyshevdist','02_ref_chebyshevdist', '04_ref_chebyshevdist',\n",
    "            '01_ref_gpdist', '04_ref_gpdist', '08_ref_gpdist','09_ref_gpdist']\n",
    "\n",
    "# Generating the dataset\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "dataframe_training = all_training_data[all_training_data['node'].isin(eval_sensors)]\n",
    "\n",
    "X_data_training = np.full((len(dataframe_training), 16), -111.)\n",
    "# Temporal information\n",
    "X_data_training[:, 0] = dataframe_training['moy_sin']\n",
    "X_data_training[:, 1] = dataframe_training['moy_cos']\n",
    "X_data_training[:, 2] = dataframe_training['dow_sin']\n",
    "X_data_training[:, 3] = dataframe_training['dow_cos']\n",
    "X_data_training[:, 4] = dataframe_training['seconds_from_midnight_sin']\n",
    "X_data_training[:, 5] = dataframe_training['seconds_from_midnight_cos']\n",
    "\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    X_data_training[:, i+6] = array_rep(all_training_data[all_training_data['node'] == ref_sensor]['temperature'], len(eval_sensors))\n",
    "    \n",
    "# X and Y coords\n",
    "X_data_training[:, 10] = dataframe_training['coord_x']\n",
    "X_data_training[:, 11] = dataframe_training['coord_y']\n",
    "\n",
    "\n",
    "# # angular distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "#     X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "#     sens_points = list(zip(X_coord,Y_coord))\n",
    "#     X_data_training[:, 12+i] = angular_distance(np.array(ref_points), np.array(sens_points))\n",
    "\n",
    "    \n",
    "# # euclidean distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "#     X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "#     sens_points = list(zip(X_coord,Y_coord))\n",
    "#     X_data_training[:, 16+i] = euclidean_distance(np.array(ref_points), np.array(sens_points))\n",
    "    \n",
    "    \n",
    "# # manhattan distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "#     X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "#     sens_points = list(zip(X_coord,Y_coord))\n",
    "#     X_data_training[:, 20+i] = manhattan_distance(np.array(ref_points), np.array(sens_points))\n",
    "    \n",
    "# # chebyshev distances\n",
    "# for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "#     X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "#     ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "#     X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "#     sens_points = list(zip(X_coord,Y_coord))\n",
    "#     X_data_training[:, 24+i] = chebyshev_distance(np.array(ref_points), np.array(sens_points))\n",
    "        \n",
    "# genetic distances\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_data_training[:, 12+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "\n",
    "assert np.min(X_data_training) > -111.\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_data_training = scaler.fit_transform(X_data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating correlations\n",
    "features = ['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', \n",
    "            'seconds_from_midnight_sin', 'seconds_from_midnight_cos',\n",
    "            '01_ref_temp', '04_ref_temp', '08_ref_temp','09_ref_temp',\n",
    "            'X_coord', 'Y_coord',\n",
    "#             '01_ref_angulardist', '08_ref_angulardist','02_ref_angulardist', '04_ref_angulardist',\n",
    "#             '01_ref_euclideandist', '08_ref_euclideandist','02_ref_euclideandist', '04_ref_euclideandist',\n",
    "#             '01_ref_manhattandist', '08_ref_manhattandist','02_ref_manhattandist', '04_ref_manhattandist',\n",
    "#             '01_ref_chebyshevdist', '08_ref_chebyshevdist','02_ref_chebyshevdist', '04_ref_chebyshevdist',\n",
    "            '01_ref_gpdist', '04_ref_gpdist', '08_ref_gpdist','09_ref_gpdist']\n",
    "\n",
    "corr_matrix = pd.DataFrame(np.corrcoef(X_data_training, rowvar=False), columns=features, index=features)\n",
    "corr_matrix = corr_matrix.abs()\n",
    "\n",
    "list_corrs = pd.DataFrame(corr_matrix.unstack().sort_values(ascending=False)).reset_index()\n",
    "list_corrs.columns = ['feat_1', 'feat_2', 'corr']\n",
    "list_corrs = list_corrs[list_corrs['feat_1'] < list_corrs['feat_2']]\n",
    "\n",
    "print(list_corrs[0:20])\n",
    "\n",
    "\n",
    "break_corrs = []\n",
    "   \n",
    "            \n",
    "break_corrs_idx = []\n",
    "for col in break_corrs:\n",
    "    break_corrs_idx.append(features.index(col))\n",
    "    \n",
    "features_no_corr = [x for x in features if x not in break_corrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data_training_no_corr = X_data_training[:, [x for x in range(len(features)) if x not in break_corrs_idx ]]\n",
    "\n",
    "corr_matrix = pd.DataFrame(np.corrcoef(X_data_training_no_corr, rowvar=False), columns=features_no_corr, index=features_no_corr)\n",
    "corr_matrix = corr_matrix.abs()\n",
    "\n",
    "list_corrs = pd.DataFrame(corr_matrix.unstack().sort_values(ascending=False)).reset_index()\n",
    "list_corrs.columns = ['feat_1', 'feat_2', 'corr']\n",
    "list_corrs = list_corrs[list_corrs['feat_1'] < list_corrs['feat_2']]\n",
    "\n",
    "print(list_corrs[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with SHAP and XGBoost, in full-training over 9 sensors (all - ref)\n",
    "# I predict just the single temperature value\n",
    "# https://medium.com/@lucasramos_34338/visualizing-variable-importance-using-shap-and-cross-validation-bd5075e9063a\n",
    "\n",
    "\n",
    "# Calculate the SHAP values\n",
    "shap_vals = []\n",
    "shap_insts = []\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import shap\n",
    "reg = XGBRegressor(n_jobs=31, random_state=42).fit(X_data_training_no_corr, dataframe_training['temperature'].values) #n_estimators=100 (default)\n",
    "\n",
    "subsampled = shap.sample(X_data_training_no_corr, 1500, random_state=42)\n",
    "explainer = shap.TreeExplainer(reg, subsampled)\n",
    "shap_values = explainer.shap_values(subsampled)\n",
    "shap_vals.extend(shap_values)\n",
    "shap_insts.extend(subsampled)\n",
    "\n",
    "pickle_out = open('shap_vals.pickle',\"wb\")\n",
    "pickle.dump(shap_vals, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('shap_insts.pickle',\"wb\")\n",
    "pickle.dump(shap_insts, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "pickle_out = open('shap_vals.pickle',\"rb\")\n",
    "shap_vals = np.asarray(pickle.load(pickle_out))\n",
    "pickle_out.close()  \n",
    "\n",
    "pickle_out = open('shap_insts.pickle',\"rb\")\n",
    "shap_insts = np.asarray(pickle.load(pickle_out))\n",
    "pickle_out.close()\n",
    "\n",
    "shap.summary_plot(shap_vals, shap_insts, feature_names=features_no_corr, show=False)\n",
    "plt.savefig(\"shap.pdf\", format='pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_no_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (reference sensors and some other data)\n",
    "# EVALUATION\n",
    "\n",
    "to_delete = [x for x in results['method'] if 'linreg' in x]\n",
    "results = results[np.logical_not(results['method'].isin(to_delete))]\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# selected features\n",
    "# ['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', 'seconds_from_midnight_sin', 'seconds_from_midnight_cos', '02_ref_temp', '10_ref_temp', '05_ref_temp', '02_ref_dist', '10_ref_dist', '05_ref_dist', '02_mutual_center', '10_mutual_center', '05_mutual_center', 'min_window_dist', 'X_coord', 'Y_coord'\n",
    "\n",
    "\n",
    "# Based on the selected features, I now try LinearRegression\n",
    "# The evaluation is performed by means of leave-one-out cross-validation\n",
    "# I predict just the single temperature value\n",
    "\n",
    "#['raspihat02', 'raspihat10', 'raspihat11'] 1.4782938271044614\n",
    "#['raspihat02', 'raspihat10', 'raspihat05'] 1.3759392664427377\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "\n",
    "# Data that is common for each fold (training)\n",
    "common_data_training = np.full((training_data_ts, 10), -111.)\n",
    "# Temporal information\n",
    "common_data_training[:, 0] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_sin']\n",
    "common_data_training[:, 1] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_training[:, 2] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_sin']\n",
    "common_data_training[:, 3] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_training[:, 4] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_training[:, 5] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    common_data_training[:, i+6] = all_training_data[all_training_data['node'] == ref_sensor]['temperature']\n",
    "\n",
    "\n",
    "# Data that is common for each fold (test)\n",
    "common_data_test = np.full((test_data_ts, 10), -111.)\n",
    "# Temporal information\n",
    "common_data_test[:, 0] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_sin']\n",
    "common_data_test[:, 1] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_test[:, 2] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_sin']\n",
    "common_data_test[:, 3] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_test[:, 4] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_test[:, 5] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    common_data_test[:, i+6] = all_test_data[all_test_data['node'] == ref_sensor]['temperature']\n",
    "\n",
    "fold_sensors = sorted(eval_sensors)\n",
    "fold_data_training = all_training_data[all_training_data['node'].isin(fold_sensors)]\n",
    "\n",
    "X_train_data_fold = np.full((training_data_ts*len(fold_sensors), 16), -111.)\n",
    "    \n",
    "# Copy the common data\n",
    "X_train_data_fold[:, 0:10] = array_rep(common_data_training, len(fold_sensors))\n",
    "\n",
    "# X, Y coord\n",
    "X_train_data_fold[:, 10] = fold_data_training['coord_x']\n",
    "X_train_data_fold[:, 11] = fold_data_training['coord_y']\n",
    "\n",
    "\n",
    "# Weighted distance\n",
    "for i, ref_sensor in enumerate(ref_sensors_here[0:4]):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = fold_data_training['coord_x'].values, fold_data_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_train_data_fold[:, 12+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "\n",
    "assert np.min(X_train_data_fold) > -111.\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_data_fold=scaler.fit_transform(X_train_data_fold)\n",
    "\n",
    "# Train the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(n_jobs=31).fit(X_train_data_fold, fold_data_training['temperature'].values)\n",
    "\n",
    "\n",
    "# Now I perform the K-fold cross-validation\n",
    "pbar = tqdm(total=len(eval_sensors), desc=\"FOLDS done\")\n",
    "for sens_index, sensor in enumerate(eval_sensors):\n",
    "    fold_data_test = all_test_data[all_test_data['node'] == sensor]\n",
    "    \n",
    "    X_test_data_fold = np.full((test_data_ts, 16), -111.)\n",
    "    \n",
    "    # Copy the common data\n",
    "    X_test_data_fold[:, 0:10] = common_data_test\n",
    "    \n",
    "    # X, Y coord\n",
    "    X_test_data_fold[:, 10] = fold_data_test['coord_x']   \n",
    "    X_test_data_fold[:, 11] = fold_data_test['coord_y']\n",
    "\n",
    "    for i, ref_sensor in enumerate(ref_sensors_here[0:4]):\n",
    "        X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "        ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "        X_coord, Y_coord = fold_data_test['coord_x'].values, fold_data_test['coord_y'].values\n",
    "        sens_points = list(zip(X_coord,Y_coord))\n",
    "        X_test_data_fold[:, 12+i] = gp_distance(ref_points, sens_points)    \n",
    "    \n",
    "    assert np.min(X_test_data_fold) > -111.\n",
    "\n",
    "    # Normalizing predictors\n",
    "    X_test_data_fold=scaler.transform(X_test_data_fold)\n",
    "    \n",
    "    \n",
    "    # Predict the values, and evaluate the errors\n",
    "    fold_preds = reg.predict(X_test_data_fold) \n",
    "    \n",
    "    errors = abs(fold_data_test['temperature'].values - fold_preds)\n",
    "    \n",
    "    median_error = np.percentile(errors, 50)\n",
    "    p95_error = np.percentile(errors, 95)\n",
    "    results.loc[len(results)] = ['linreg', sensor, median_error, p95_error]\n",
    "    \n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()\n",
    "\n",
    "results.sort_values(by=['method', 'fold'], inplace=True)\n",
    "\n",
    "pickle_out = open('results.pickle',\"wb\")\n",
    "pickle.dump(results, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()\n",
    "display(results.groupby('method')[['method', '95_perc_error']].median().sort_values(by=['95_perc_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Tuning, performed on fixed training-test split, since cross-validation would be too costly\n",
    "\n",
    "#### XGBoost: TUNING\n",
    "# Tuning is performed on a single training/validation split, since leave-one-out cross-validation\n",
    "# would be too costly\n",
    "\n",
    "# Generating the dataset\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "dataframe_training = all_training_data[all_training_data['node'].isin(eval_sensors)]\n",
    "\n",
    "X_data = np.full((len(dataframe_training), 11), -111.)\n",
    "# Temporal information\n",
    "X_data[:, 0] = dataframe_training['moy_sin']\n",
    "# X_data[:, 1] = dataframe_training['moy_cos']\n",
    "X_data[:, 1] = dataframe_training['dow_sin']\n",
    "# X_data[:, 3] = dataframe_training['dow_cos']\n",
    "X_data[:, 2] = dataframe_training['seconds_from_midnight_sin']\n",
    "X_data[:, 3] = dataframe_training['seconds_from_midnight_cos']\n",
    "\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    X_data[:, i+4] = array_rep(all_training_data[all_training_data['node'] == ref_sensor]['temperature'], len(eval_sensors))\n",
    "\n",
    "X_data[:, 8] = dataframe_training['coord_x']\n",
    "X_data[:, 9] = dataframe_training['coord_y']\n",
    "\n",
    "for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = dataframe_training['coord_x'].values, dataframe_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_data[:, 10+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "assert np.min(X_data) > -111.\n",
    "\n",
    "y_data = dataframe_training['temperature'].values\n",
    "\n",
    "# Splitting into training and eval data (week based splitting)\n",
    "np.random.seed(42)\n",
    "weeks_list = dataframe_training.year_week.unique().tolist()\n",
    "np.random.shuffle(weeks_list)\n",
    "train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc_training)])\n",
    "\n",
    "# Different weeks between training and validation...\n",
    "X_data_training = X_data[dataframe_training['year_week'].isin(train_weeks)]\n",
    "y_data_training = y_data[dataframe_training['year_week'].isin(train_weeks)]\n",
    "\n",
    "X_data_validation = X_data[~ dataframe_training['year_week'].isin(train_weeks)]\n",
    "y_data_validation = y_data[~ dataframe_training['year_week'].isin(train_weeks)]\n",
    "\n",
    "print(X_data_training.shape, X_data_validation.shape)\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_data_training=scaler.fit_transform(X_data_training)\n",
    "X_data_validation=scaler.transform(X_data_validation)\n",
    "\n",
    "# Tuning with hyperopt\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    model = XGBRegressor(n_jobs=31, random_state=42, **params).fit(X_data_training, y_data_training)\n",
    "    predictions = model.predict(X_data_validation)\n",
    "    errors = np.abs(y_data_validation - predictions)\n",
    "    return np.percentile(errors, 95)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1, 200, 5)),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175]),\n",
    "    'n_estimators': hp.choice('n_estimators', range(250, 700, 50)),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha', 0, 100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "    'gamma': hp.uniform ('gamma', 0, 9),\n",
    "    'subsample' : hp.uniform('subsample', 0.6, 1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.6, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1)\n",
    "}\n",
    "\n",
    "\n",
    "tuning_history = []\n",
    "\n",
    "best = 9999999\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = hyperopt_train_test(params)\n",
    "    if acc < best:\n",
    "        best = acc\n",
    "        print('new best:', best, params)\n",
    "        tuning_history.append((best, params))\n",
    "        \n",
    "        pickle_file = open('tuning_history.pickle',\"wb\")\n",
    "        pickle.dump(tuning_history, pickle_file)\n",
    "        pickle_file.close()    \n",
    "        \n",
    "    return {'loss': acc, 'status': STATUS_OK} \n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, search_space, algo=tpe.suggest, max_evals=40, trials=trials) # minimizza lo score\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### XGBoost: EVALUATION\n",
    "\n",
    "ref_sensors_here = ['raspihat08','raspihat09','raspihat01','raspihat04']\n",
    "\n",
    "to_delete = [x for x in results['method'] if 'xgboost' in x]\n",
    "results = results[np.logical_not(results['method'].isin(to_delete))]\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "eval_sensors = sorted(list(set(all_sensors) - set(ref_sensors_here)))\n",
    "\n",
    "\n",
    "# Data that is common for each fold (training)\n",
    "common_data_training = np.full((training_data_ts, 8), -111.)\n",
    "# Temporal information\n",
    "common_data_training[:, 0] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_sin']\n",
    "# common_data_training[:, 1] = all_training_data[all_training_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_training[:, 1] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_sin']\n",
    "# common_data_training[:, 3] = all_training_data[all_training_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_training[:, 2] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_training[:, 3] = all_training_data[all_training_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    common_data_training[:, i+4] = all_training_data[all_training_data['node'] == ref_sensor]['temperature']\n",
    "\n",
    "\n",
    "# Data that is common for each fold (test)\n",
    "common_data_test = np.full((test_data_ts, 8), -111.)\n",
    "# Temporal information\n",
    "common_data_test[:, 0] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_sin']\n",
    "# common_data_test[:, 1] = all_test_data[all_test_data['node'] == 'raspihat01']['moy_cos']\n",
    "common_data_test[:, 1] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_sin']\n",
    "# common_data_test[:, 3] = all_test_data[all_test_data['node'] == 'raspihat01']['dow_cos']\n",
    "common_data_test[:, 2] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_sin']\n",
    "common_data_test[:, 3] = all_test_data[all_test_data['node'] == 'raspihat01']['seconds_from_midnight_cos']\n",
    "# Reference temperatures\n",
    "for i, ref_sensor in enumerate(ref_sensors_here):\n",
    "    common_data_test[:, i+4] = all_test_data[all_test_data['node'] == ref_sensor]['temperature']\n",
    "    \n",
    "fold_sensors = sorted(eval_sensors)\n",
    "fold_data_training = all_training_data[all_training_data['node'].isin(fold_sensors)]\n",
    "\n",
    "X_train_data_fold = np.full((training_data_ts*len(fold_sensors), 11), -111.)\n",
    "    \n",
    "# Copy the common data\n",
    "X_train_data_fold[:, 0:8] = array_rep(common_data_training, len(fold_sensors))\n",
    "\n",
    "# X and Y coord ['raspihat01','raspihat04','raspihat08','raspihat09']\n",
    "X_train_data_fold[:, 8] = fold_data_training['coord_x']\n",
    "X_train_data_fold[:, 9] = fold_data_training['coord_y']\n",
    "\n",
    "\n",
    "# Weighted distance\n",
    "for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "    X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "    ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "    X_coord, Y_coord = fold_data_training['coord_x'].values, fold_data_training['coord_y'].values\n",
    "    sens_points = list(zip(X_coord,Y_coord))\n",
    "    X_train_data_fold[:, 10+i] = gp_distance(ref_points, sens_points)\n",
    "\n",
    "assert np.min(X_train_data_fold) > -111.\n",
    "\n",
    "\n",
    "# Normalizing predictors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_data_fold=scaler.fit_transform(X_train_data_fold)\n",
    "\n",
    "# Train the model\n",
    "from xgboost import XGBRegressor    \n",
    "\n",
    "reg = XGBRegressor(n_jobs=31, random_state=42).fit(X_train_data_fold, fold_data_training['temperature'].values)\n",
    "                   \n",
    "print(\"Model trained.\")\n",
    "\n",
    "# Now I perform the evaluation on eval sensors\n",
    "pbar = tqdm(total = len(eval_sensors))\n",
    "\n",
    "for sens_index, sensor in enumerate(eval_sensors):\n",
    "\n",
    "    fold_data_test = all_test_data[all_test_data['node'] == sensor]\n",
    "    X_test_data_fold = np.full((test_data_ts, 11), -111.)\n",
    "    \n",
    "    # Copy the common data\n",
    "    X_test_data_fold[:, 0:8] = common_data_test\n",
    "    \n",
    "    # X and Y coord\n",
    "    X_test_data_fold[:, 8] = fold_data_test['coord_x']\n",
    "    X_test_data_fold[:, 9] = fold_data_test['coord_y']\n",
    "\n",
    "    \n",
    "    # Weighted distance\n",
    "    for i, ref_sensor in enumerate(ref_sensors_here[0:1]):\n",
    "        X_coord_ref, Y_coord_ref = node_map_coords[ref_sensor]\n",
    "        ref_points = [(X_coord_ref, Y_coord_ref)]\n",
    "        X_coord, Y_coord = fold_data_test['coord_x'].values, fold_data_test['coord_y'].values\n",
    "        sens_points = list(zip(X_coord,Y_coord))\n",
    "        X_test_data_fold[:, 10+i] = gp_distance(ref_points, sens_points)\n",
    "    \n",
    "    assert np.min(X_test_data_fold) > -111.\n",
    "    \n",
    "    # Normalizing predictors\n",
    "    X_test_data_fold = scaler.transform(X_test_data_fold)\n",
    "    \n",
    "    # Predict the values, and evaluate the errors\n",
    "    fold_preds = reg.predict(X_test_data_fold)\n",
    "    \n",
    "    errors = abs(fold_data_test['temperature'].values - fold_preds)\n",
    "    \n",
    "    median_error = np.percentile(errors, 50)\n",
    "    p95_error = np.percentile(errors, 95)\n",
    "    results.loc[len(results)] = ['xgboost', sensor, median_error, p95_error]\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "results.sort_values(by=['method', 'fold'], inplace=True)\n",
    "\n",
    "pickle_out = open('results.pickle',\"wb\")\n",
    "pickle.dump(results, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
