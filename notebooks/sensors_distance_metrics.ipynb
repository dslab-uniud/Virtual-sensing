{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from scipy.integrate import simps\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import array\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import gp\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "\n",
    "# !pip install kneed \n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file):\n",
    "    try:\n",
    "        pickle_in= open(file, \"rb\")\n",
    "        data = pickle.load(pickle_in)\n",
    "        pickle_in.close()\n",
    "        return data\n",
    "    except IOError:\n",
    "        print(\"Error loading pickle data from \" + file)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def write_pickle(data, file):    \n",
    "    try:\n",
    "        pickle_out = open(file, \"wb\")\n",
    "        pickle.dump(data, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle_out.close()\n",
    "        return True\n",
    "    except IOError:\n",
    "        print(\"Error writing data to \" + file)\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "#genetic distance utility functions\n",
    "# Define fitness function and individuals structure\n",
    "creator.create(\"fitness\", base.Fitness, weights=(-1.0,)) # minimize a single objective (-1) or maximize it (+1)\n",
    "\n",
    "\n",
    "def my_sqrt(arg1):\n",
    "    return np.nan_to_num(np.sqrt(arg1))\n",
    "def my_log(arg1):\n",
    "    return np.nan_to_num(np.log(arg1))\n",
    "def my_abs(arg1):\n",
    "    return np.nan_to_num(np.abs(arg1))\n",
    "def my_neg(arg1):\n",
    "    return np.nan_to_num(np.negative(arg1))\n",
    "def my_square(arg1):\n",
    "    return np.nan_to_num(np.square(arg1))\n",
    "def my_add(arg1, arg2):\n",
    "    return np.nan_to_num(np.add(arg1, arg2))\n",
    "def my_sub(arg1, arg2):\n",
    "    return np.nan_to_num(np.subtract(arg1, arg2))\n",
    "def my_mul(arg1, arg2):\n",
    "    return np.nan_to_num(np.multiply(arg1, arg2))\n",
    "def my_div(arg1, arg2):   \n",
    "    return np.nan_to_num(np.divide(arg1, arg2))\n",
    "def my_pow(arg1, arg2):\n",
    "    return np.nan_to_num(np.power(float(arg1), float(arg2)))\n",
    "def my_max(arg1, arg2):\n",
    "    return np.nan_to_num(np.maximum(arg1, arg2), nan=sys.float_info.min)\n",
    "def my_min(arg1, arg2):\n",
    "    return np.nan_to_num(np.minimum(arg1, arg2), nan=sys.float_info.max)\n",
    "\n",
    "\n",
    "pset = gp.PrimitiveSet(\"MAIN\", arity=3)\n",
    "pset.addPrimitive(my_add, 2)\n",
    "pset.addPrimitive(my_sub, 2)\n",
    "pset.addPrimitive(my_mul, 2)\n",
    "pset.addPrimitive(my_div, 2)\n",
    "pset.addPrimitive(my_pow, 2)\n",
    "pset.addPrimitive(my_max, 2)\n",
    "pset.addPrimitive(my_min, 2)\n",
    "pset.addPrimitive(my_log, 1)\n",
    "pset.addPrimitive(my_sqrt, 1)\n",
    "pset.addPrimitive(my_square, 1)\n",
    "pset.addPrimitive(my_abs, 1)\n",
    "pset.addPrimitive(my_neg, 1)\n",
    "\n",
    "pset.addEphemeralConstant(\"my_weight_1\", lambda: random.uniform(-1, 1))\n",
    "pset.addEphemeralConstant(\"my_weight_2\", lambda: random.uniform(-1, 1))\n",
    "pset.addEphemeralConstant(\"my_weight_3\", lambda: random.uniform(-1, 1))\n",
    "# pset.addEphemeralConstant(\"my_weight_4\", lambda: random.uniform(-1, 1))\n",
    "# pset.addEphemeralConstant(\"my_weight_5\", lambda: random.uniform(-1, 1))\n",
    "\n",
    "\n",
    "# Now create the Individual class\n",
    "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.fitness, pset=pset)\n",
    "toolbox = base.Toolbox() # this is a containr for all individuals, functions, operators, etc.\n",
    "\n",
    "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=5) # generator function to initialize the population individuals. \"genHalfAndHalf\" generates the trees in a list format, parameters: min_height, max_height\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr) # initializer for the elements of the population: a weight for each fingerprint distance function\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) # initializer for the population\n",
    "toolbox.register(\"compile\", gp.compile, pset=pset) # needed to compile the mathematical expressions\n",
    "\n",
    "# Load best genetic programming function\n",
    "# results = load_pickle('genetic_partial_results.pickle')\n",
    "# gp_func=toolbox.compile(expr=list(results)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataframe from disk, if it exists\n",
    "data = load_pickle('extended_dataframe.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I delete all days containing NaN values\n",
    "# Better to do proceed in this way, so not to have strange cut points\n",
    "\n",
    "rows_with_nans = np.where(np.isnan(data['temperature'].values))[0]\n",
    "dates_with_nans = np.unique(data['only_date'][rows_with_nans])\n",
    "dates_indexes = data.index[data['only_date'].isin(dates_with_nans)].tolist()\n",
    "print(\"Percentuale di dati scartati:\", len(dates_indexes) / len(data))\n",
    "data = data.drop(data.index[dates_indexes]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "data['year_week'] = data.datetime.dt.strftime('%Y-%U')\n",
    "\n",
    "# For each sensor, I determine the indexes to partition its data into training and test\n",
    "# In doing that, I keep into account the temporal relationships\n",
    "# Meaning, test data is in the future wrt training data\n",
    "\n",
    "perc_training = 0.8\n",
    "\n",
    "\n",
    "split_type = 'week_based_random' # temporal_wise, day_based_random, full_random\n",
    "\n",
    "    \n",
    "# Checking that every sensor has the same length\n",
    "counts_per_sensor = data[['node', 'datetime']].groupby('node').count()\n",
    "assert np.max(counts_per_sensor['datetime']) == np.min(counts_per_sensor['datetime'])\n",
    "sensor_data_length = counts_per_sensor['datetime'][0]    \n",
    "\n",
    "   \n",
    "# Maps that store the training and test indexes for each sensor\n",
    "sens_map_train_indexes = {}\n",
    "sens_map_test_indexes = {}\n",
    "\n",
    "for sens in list(counts_per_sensor.index):\n",
    "    sens_map_train_indexes[sens] = []\n",
    "    sens_map_test_indexes[sens] = []    \n",
    "    \n",
    "    \n",
    "    \n",
    "if split_type == 'temporal_wise':\n",
    "    # first 70% of each sensor data is traning, and next 30% is test\n",
    "\n",
    "    # Generate training and test indexes\n",
    "\n",
    "    # I have already sorted the values by node and datetime in the original dataframe\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i*sensor_data_length\n",
    "        last_train = base_sens + int(sensor_data_length*perc_training)\n",
    "        last_test = last_train + int(sensor_data_length*(1 - perc_training)) + 1\n",
    "        train_inds = list(range(base_sens, last_train))\n",
    "        test_inds = list(range(last_train, last_test))\n",
    "        sens_map_train_indexes[sens] = train_inds\n",
    "        sens_map_test_indexes[sens] = test_inds\n",
    "        \n",
    "    for sens in list(counts_per_sensor.index):\n",
    "        first_train = sens_map_train_indexes[sens][0]\n",
    "        last_train = sens_map_train_indexes[sens][-1]\n",
    "        first_test = sens_map_test_indexes[sens][0]\n",
    "        last_test = sens_map_test_indexes[sens][-1]\n",
    "        assert first_train == data.query(\"node == @sens\").reset_index().iloc[0,0]\n",
    "        assert last_test == data.query(\"node == @sens\").reset_index().iloc[-1,0]\n",
    "        assert last_train > first_train and last_train < first_test\n",
    "        assert first_test > last_train and first_test < last_test\n",
    "    \n",
    "elif split_type == 'full_random':\n",
    "    # divide randomly the data of the sensors\n",
    "    \n",
    "    possible_indexes = list(range(sensor_data_length))\n",
    "    random.seed(42)\n",
    "    # I make sure to use the same split for each sensor\n",
    "    # meaning, corresponding training and test indexes of the different sensors refer to the same datetimes\n",
    "    train_indexes_overall = sorted(random.sample(possible_indexes, int(sensor_data_length * perc_training)))\n",
    "    test_indexes_overall = sorted(list(set(possible_indexes) - set(train_indexes_overall)))\n",
    "    \n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        base_sens = i * sensor_data_length\n",
    "        # it is sufficent to add the baseline, since all sensor tracks have the same length\n",
    "        sens_map_train_indexes[sens] = np.asarray(train_indexes_overall) + base_sens\n",
    "        sens_map_test_indexes[sens] = np.asarray(test_indexes_overall) + base_sens\n",
    "\n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(possible_indexes)\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'day_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    days_list = data.only_date.unique().tolist()\n",
    "    np.random.shuffle(days_list)\n",
    "    random_train_days = sorted(days_list[0:int(len(days_list)*perc_training)])\n",
    "    random_test_days =  sorted(days_list[int(len(days_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.only_date.isin(random_train_days)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.only_date.isin(random_test_days)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "\n",
    "elif split_type == 'week_based_random':\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    weeks_list = data.year_week.unique().tolist()\n",
    "    np.random.shuffle(weeks_list)\n",
    "    random_train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc_training)])\n",
    "    random_test_weeks =  sorted(weeks_list[int(len(weeks_list)*perc_training):])\n",
    "\n",
    "    for i, sens in enumerate(list(counts_per_sensor.index)):\n",
    "        subdata = data[data['node'] == sens] # here the indexes are still the same as in the original frame\n",
    "        sens_map_train_indexes[sens] = subdata[subdata.year_week.isin(random_train_weeks)].index.tolist().copy()\n",
    "        sens_map_test_indexes[sens] = subdata[subdata.year_week.isin(random_test_weeks)].index.tolist().copy()        \n",
    "    \n",
    "    assert np.array_equal(sens_map_train_indexes['raspihat01'], sens_map_train_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert np.array_equal(sens_map_test_indexes['raspihat01'], sens_map_test_indexes['raspihat03'] - sensor_data_length*2)\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).intersection(sens_map_test_indexes['raspihat01'])) == 0\n",
    "    assert len(set(sens_map_train_indexes['raspihat01']).union(sens_map_test_indexes['raspihat01'])) == len(data[data['node'] == 'raspihat01'])\n",
    "    # The training and test indexes are different for each sensor, and refer to the original dataframe\n",
    "    \n",
    "else:\n",
    "    assert False, 'Unsupported split type'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general information\n",
    "\n",
    "window_coords = [(1, 0), (3, 0), (6, 0), (8, 0), (10, 0),\n",
    "                 (1, 9), (3, 9), (6, 9), (8, 9), (10, 9)]\n",
    "\n",
    "all_train_idxs = []\n",
    "for sens in sens_map_train_indexes:\n",
    "    all_train_idxs.extend(sens_map_train_indexes[sens])\n",
    "\n",
    "all_test_idxs = []\n",
    "for sens in sens_map_test_indexes:\n",
    "    all_test_idxs.extend(sens_map_test_indexes[sens])\n",
    "\n",
    "node_map_coords = {}\n",
    "for sens in sens_map_test_indexes:\n",
    "    coord_x, coord_y = data[data['node'] == sens].iloc[0][['coord_x', 'coord_y']].values\n",
    "    node_map_coords[sens] = (coord_x, coord_y)\n",
    "    \n",
    "all_sensors = np.unique(data['node'])\n",
    "\n",
    "all_training_data = data.iloc[all_train_idxs].reset_index(drop=True)\n",
    "all_test_data = data.iloc[all_test_idxs].reset_index(drop=True)\n",
    "\n",
    "def eucl_dist(x_0, y_0, x_1, y_1):\n",
    "    return np.sqrt((x_0 - x_1)**2 + (y_0 - y_1)**2)\n",
    "\n",
    "training_data_ts = len(all_training_data[all_training_data['node'] == 'raspihat01'])\n",
    "\n",
    "test_data_ts = len(all_test_data[all_test_data['node'] == 'raspihat01'])\n",
    "\n",
    "air_tube_coords = [(x, 4) for x in list(range(11))]\n",
    "\n",
    "def array_rep(arr, n):\n",
    "    assert len(arr.shape) <= 2\n",
    "    if len(arr.shape) < 2:\n",
    "        return np.tile(arr, n)\n",
    "    else:\n",
    "        retarr = np.full((arr.shape[0]*n, arr.shape[1]), -1.)\n",
    "        for i in range(n):\n",
    "            retarr[i*arr.shape[0]: (i+1)*arr.shape[0]] = arr\n",
    "        return retarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Calcolo rank distanze con metriche base\n",
    "\n",
    "# Now that we have a baseline for how to discard sensors using the temperature information,\n",
    "# Let us see how closely we can match them using proxy information (e.g., spatial info)\n",
    "\n",
    "# So, for each sensor, I now determine the rank of the other sensors\n",
    "\n",
    "\n",
    "# determines the angle between two points (x_0, y_0) (x_1, y_1)\n",
    "# if p2 is the origin (0, 0), then I get the angle wrt x axis\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
    "\n",
    "def angle_between_center(p1, p2):\n",
    "    ang1 = np.arctan2(*np.subtract(p1, (5,5))[::-1])\n",
    "    ang2 = np.arctan2(*np.subtract(p2, (5,5))[::-1])\n",
    "    return np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
    "\n",
    "# Maps store the results from the best to the worst sensor, according to the specific metric\n",
    "sensor_map_euclidean_ranks = {}\n",
    "sensor_map_angular_ranks = {}\n",
    "sensor_map_angular_center_ranks = {}\n",
    "sensor_map_manhattan_ranks = {}\n",
    "sensor_map_chebyshev_ranks = {}\n",
    "sensor_map_window_ranks = {}\n",
    "sensor_map_center_ranks = {}\n",
    "sensor_map_hdist_ranks = {}\n",
    "sensor_map_vdist_ranks = {}\n",
    "\n",
    "min_euclidean=sys.float_info.max\n",
    "max_euclidean=sys.float_info.min\n",
    "min_manhattan=sys.float_info.max\n",
    "max_manhattan=sys.float_info.min\n",
    "min_chebyshev=sys.float_info.max\n",
    "max_chebyshev=sys.float_info.min\n",
    "min_hdist=sys.float_info.max\n",
    "max_hdist=sys.float_info.min\n",
    "min_vdist=sys.float_info.max\n",
    "max_vdist=sys.float_info.min\n",
    "\n",
    "pbar = tqdm(total=len(all_sensors))\n",
    "for ind,sens in enumerate(all_sensors):\n",
    "    \n",
    "    euclidean_ranks = []\n",
    "    angular_ranks = []\n",
    "    angular_center_ranks = []\n",
    "    manhattan_ranks = []\n",
    "    chebyshev_ranks = []\n",
    "    hdist_ranks = []\n",
    "    vdist_ranks = []\n",
    "    window_ranks = []\n",
    "    center_ranks = []\n",
    "\n",
    "    sens_x, sens_y = all_training_data.query(\"node == @sens\").iloc[0][['coord_x', 'coord_y']].values\n",
    "    other_sensors = sorted(list(set(all_sensors) - set([sens])))\n",
    "    for other_sens in other_sensors: \n",
    "        other_sens_x, other_sens_y = all_training_data.query(\"node == @other_sens\").iloc[0][['coord_x', 'coord_y']].values\n",
    "        \n",
    "        angular_distance = min(angle_between((sens_x, sens_y), (other_sens_x, other_sens_y)),\n",
    "                                   angle_between((other_sens_x, other_sens_y), (sens_x, sens_y)))\n",
    "        angular_center_distance = min(angle_between_center((sens_x, sens_y), (other_sens_x, other_sens_y)),\n",
    "                                   angle_between_center((other_sens_x, other_sens_y), (sens_x, sens_y)))\n",
    "        euclidean_distance = np.sqrt((sens_x - other_sens_x)**2 + (sens_y - other_sens_y)**2)\n",
    "            \n",
    "        \n",
    "        manhattan_distance = abs(sens_x - other_sens_x) + abs(sens_y - other_sens_y)\n",
    "        chebyshev_distance = max(abs(sens_x - other_sens_x), abs(sens_y - other_sens_y))\n",
    "\n",
    "        hdist_distance = abs(sens_x - other_sens_x)\n",
    "        vdist_distance = abs(sens_y - other_sens_y)\n",
    "        \n",
    "        #updates min max values \n",
    "        if euclidean_distance < min_euclidean: min_euclidean=euclidean_distance\n",
    "        if euclidean_distance > max_euclidean: max_euclidean=euclidean_distance\n",
    "        if manhattan_distance < min_manhattan: min_manhattan=manhattan_distance\n",
    "        if manhattan_distance > max_manhattan: max_manhattan=manhattan_distance\n",
    "        if chebyshev_distance < min_chebyshev: min_chebyshev=chebyshev_distance\n",
    "        if chebyshev_distance > max_chebyshev: max_chebyshev=chebyshev_distance\n",
    "        if hdist_distance < min_hdist: min_hdist=hdist_distance\n",
    "        if hdist_distance > max_hdist: max_hdist=hdist_distance\n",
    "        if vdist_distance < min_vdist: min_vdist=vdist_distance\n",
    "        if vdist_distance > max_vdist: max_vdist=vdist_distance\n",
    "            \n",
    "\n",
    "        # the degree of similarity between two points, concerning their vicinity degree to a window\n",
    "        sens_min_window_distance = min([np.sqrt((sens_x - w_x)**2 + (sens_y - w_y)**2) for (w_x, w_y) in window_coords])\n",
    "        sens_other_min_window_distance = min([np.sqrt((other_sens_x - w_x)**2 + (other_sens_y - w_y)**2) for (w_x, w_y) in window_coords])\n",
    "        window_distance_similarity = abs(sens_min_window_distance - sens_other_min_window_distance)\n",
    "        \n",
    "        # the degree of similarity between two points, concerning their vicinity to the center (5, 5)\n",
    "        sens_center_distance = np.sqrt((sens_x - 5)**2 + (sens_y - 5)**2)\n",
    "        sens_other_center_distance = np.sqrt((other_sens_x - 5)**2 + (other_sens_y - 5)**2)\n",
    "        center_distance_similarity = abs(sens_center_distance - sens_other_center_distance)\n",
    "        \n",
    "        \n",
    "        angular_ranks.append(angular_distance)\n",
    "        angular_center_ranks.append(angular_center_distance)\n",
    "        euclidean_ranks.append(euclidean_distance)\n",
    "        manhattan_ranks.append(manhattan_distance)\n",
    "        chebyshev_ranks.append(chebyshev_distance)\n",
    "        hdist_ranks.append(hdist_distance)\n",
    "        vdist_ranks.append(vdist_distance)\n",
    "        window_ranks.append(window_distance_similarity)\n",
    "        center_ranks.append(center_distance_similarity)\n",
    "\n",
    "        \n",
    "    sensor_map_angular_ranks[sens] = [(x,y) for y,x in sorted(zip(angular_ranks, other_sensors), reverse=False)]    \n",
    "    sensor_map_angular_center_ranks[sens] = [(x,y) for y,x in sorted(zip(angular_center_ranks, other_sensors), reverse=False)]\n",
    "    sensor_map_euclidean_ranks[sens] = [(x,y) for y,x in sorted(zip(euclidean_ranks, other_sensors), reverse=False)]\n",
    "    sensor_map_manhattan_ranks[sens] = [(x,y) for y,x in sorted(zip(manhattan_ranks, other_sensors), reverse=False)]\n",
    "    sensor_map_chebyshev_ranks[sens] = [(x,y) for y,x in sorted(zip(chebyshev_ranks, other_sensors), reverse=False)]  \n",
    "    sensor_map_hdist_ranks[sens] = [(x,y) for y,x in sorted(zip(hdist_ranks, other_sensors), reverse=False)]  \n",
    "    sensor_map_vdist_ranks[sens] = [(x,y) for y,x in sorted(zip(vdist_ranks, other_sensors), reverse=False)]\n",
    "    sensor_map_window_ranks[sens] = [(x,y) for y,x in sorted(zip(window_ranks, other_sensors), reverse=False)]  \n",
    "    sensor_map_center_ranks[sens] = [(x,y) for y,x in sorted(zip(center_ranks, other_sensors), reverse=False)]\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare datasets map for each sensors  \n",
    "\n",
    "sensor_trainset = {} \n",
    "sensor_evalset = {} \n",
    "sensor_testset = {} \n",
    "\n",
    "sensor_cols = {}\n",
    "perc=.8 #for genetic train/eval split\n",
    "\n",
    "for sens in all_sensors:\n",
    "\n",
    "    ## Preparing Training and Test Data\n",
    "    \n",
    "    X_train = all_training_data[all_training_data['node'] == sens]\n",
    "    X_test = all_test_data[all_test_data['node'] == sens]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    weeks_list = X_train.year_week.unique().tolist()\n",
    "    np.random.shuffle(weeks_list)\n",
    "    train_weeks = sorted(weeks_list[0:int(len(weeks_list)*perc)])\n",
    "\n",
    "    X_eval = X_train[~ (X_train.year_week.isin(train_weeks))]\n",
    "    X_train = X_train[X_train.year_week.isin(train_weeks)]\n",
    "    \n",
    "    # Temporal information + x y coords\n",
    "    X_train = X_train[['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', 'seconds_from_midnight_sin', 'seconds_from_midnight_cos', 'coord_x', 'coord_y']]\n",
    "    X_eval = X_eval[['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', 'seconds_from_midnight_sin', 'seconds_from_midnight_cos', 'coord_x', 'coord_y']]\n",
    "    X_test = X_test[['moy_sin', 'moy_cos', 'dow_sin', 'dow_cos', 'seconds_from_midnight_sin', 'seconds_from_midnight_cos', 'coord_x', 'coord_y']] \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Reference temperatures\n",
    "    for ref_sensor in all_sensors:\n",
    "        X_train[ref_sensor+'_temperature'] = all_training_data[(all_training_data['node'] == ref_sensor) & (all_training_data.year_week.isin(train_weeks))]['temperature'].values\n",
    "        X_eval[ref_sensor+'_temperature'] = all_training_data[(all_training_data['node'] == ref_sensor) & (~ all_training_data.year_week.isin(train_weeks))]['temperature'].values\n",
    "        X_test[ref_sensor+'_temperature'] = all_test_data[all_test_data['node'] == ref_sensor]['temperature'].values\n",
    "\n",
    "    \n",
    "    # In Loving memory of our columns names\n",
    "    sensor_cols[sens] = X_train.columns\n",
    "    \n",
    "    y_train = X_train[sens+'_temperature'].values\n",
    "    y_eval = X_eval[sens+'_temperature'].values\n",
    "    y_test = X_test[sens+'_temperature'].values\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train) #RIP dataframe => RIP columns names\n",
    "    if X_eval.shape[0] > 0: \n",
    "        X_eval = scaler.transform(X_eval) #RIP dataframe => RIP columns names\n",
    "    X_test = scaler.transform(X_test) #RIP dataframe => RIP columns names\n",
    "    \n",
    "    # restore y col values\n",
    "    X_train[:, list(sensor_cols[sens]).index(sens+'_temperature')] = y_train\n",
    "    if X_eval.shape[0] > 0: \n",
    "        X_eval[:, list(sensor_cols[sens]).index(sens+'_temperature')] = y_eval\n",
    "    X_test[:, list(sensor_cols[sens]).index(sens+'_temperature')] = y_test    \n",
    "    \n",
    "    sensor_trainset[sens] = X_train\n",
    "    sensor_evalset[sens] = X_eval\n",
    "    sensor_testset[sens] = X_test\n",
    "\n",
    "# END prepare datasets map for each sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute distance metric GENETIC Programming algorithm (New version with GP tree)\n",
    "\n",
    "# Now let us turn to the genetic algorithm \n",
    "\n",
    "import random\n",
    "random.seed(20885520)#572948150)#33087541)#355118678419#350145987#4296546)#42#572948150\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from operator import attrgetter\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "reset_maps = False\n",
    "\n",
    "ranks = {\n",
    "#          'angular':sensor_map_angular_ranks,\n",
    "         'euclidean':sensor_map_euclidean_ranks,\n",
    "         'manhattan':sensor_map_manhattan_ranks,\n",
    "         'chebyshev':sensor_map_chebyshev_ranks\n",
    "}\n",
    "\n",
    "# Now define the genetic algorithm\n",
    "\n",
    "if reset_maps:\n",
    "    fitness_map={}\n",
    "    predictors_results={}\n",
    "                         \n",
    "# Now we define the evaluation function for each individual\n",
    "def evalIndividual(parameters):\n",
    "    global fitness_map, predictors_results\n",
    "    \n",
    "    individual = parameters[0]\n",
    "    func = toolbox.compile(expr=individual)\n",
    "                         \n",
    "    results_k_sens=np.array([0.]*(len(all_sensors)-1))\n",
    "    \n",
    "    \n",
    "    for ind, sens in enumerate(all_sensors):\n",
    "        # prepare the combined rank\n",
    "        other_sens = sorted(set(all_sensors) - set([sens]))\n",
    "        sensor_corrs = []\n",
    "        for j in range(len(other_sens)):\n",
    "            sensor_corrs+=[func(#sorted(ranks['angular'][sens])[j][1], \n",
    "                                (sorted(ranks['euclidean'][sens])[j][1]-min_euclidean)/(max_euclidean - min_euclidean),\n",
    "                                (sorted(ranks['manhattan'][sens])[j][1]-min_manhattan)/(max_manhattan - min_manhattan),\n",
    "                                (sorted(ranks['chebyshev'][sens])[j][1]-min_chebyshev)/(max_chebyshev - min_chebyshev))]\n",
    "                         \n",
    "#         print('Distance estimation', sensor_corrs)\n",
    "        # sort the correlations to obtain a rank from worst (higher distance) to best\n",
    "        sensor_corrs = zip(other_sens,sensor_corrs)\n",
    "        sensor_corrs = sorted(sensor_corrs, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        rank_key=','.join([x[0] for x in sensor_corrs])\n",
    "        if rank_key in fitness_map:\n",
    "            # fitness already computed for this ranking\n",
    "            local_k_sens_results=fitness_map[rank_key]\n",
    "        else:\n",
    "            # we still have to compute the fitness for this sensors ranking\n",
    "            local_k_sens_results=[]\n",
    "            for k in range(0, len(all_sensors)-1): # from no discarded sensor to all but one discarded sensors\n",
    "\n",
    "                ## preparing the dataset\n",
    "                sensors_to_avoid = [x for (x, _) in sensor_corrs[0:k]]\n",
    "\n",
    "                ## extrapolate trainset and testset from traindata and testdata                \n",
    "                predictors_cols = [col for col in sensor_cols[sens] if (not sens in col) and not any(sensor_to_avoid in col for sensor_to_avoid in sensors_to_avoid)] \n",
    "#                 print('predictors_cols:', predictors_cols)\n",
    "\n",
    "                predictors_key=sens+':'+','.join(sorted(predictors_cols))\n",
    "    \n",
    "                if predictors_key in predictors_results:\n",
    "                    local_k_sens_results += [predictors_results[predictors_key]]\n",
    "                else:\n",
    "                    X_train = sensor_trainset[sens][:,  [list(sensor_cols[sens]).index(x) for x in predictors_cols ]]\n",
    "                    X_eval = sensor_evalset[sens][:, [list(sensor_cols[sens]).index(x) for x in predictors_cols ]]\n",
    "                    y_train = sensor_trainset[sens][:, list(sensor_cols[sens]).index(sens+'_temperature')]\n",
    "                    y_eval = sensor_evalset[sens][:, list(sensor_cols[sens]).index(sens+'_temperature')]\n",
    "\n",
    "#                     reg = LinearRegression(n_jobs=31).fit(X_train, y_train)\n",
    "                    reg = XGBRegressor(n_jobs=31, random_state=42).fit(X_train, y_train)\n",
    "                    predictions = reg.predict(X_eval)\n",
    "\n",
    "                    errors = np.abs(y_eval - predictions)\n",
    "\n",
    "                    ecdf_x = np.sort(errors)\n",
    "                    ecdf_y = np.arange(1, len(ecdf_x) + 1) / len(ecdf_x)\n",
    "                    perc_95 = int(len(ecdf_x)*0.95)\n",
    "                    ecdf_area_95 = np.trapz(ecdf_y[:perc_95], ecdf_x[:perc_95])\n",
    "\n",
    "                    local_k_sens_results += [ecdf_area_95]\n",
    "                    predictors_results[predictors_key] = ecdf_area_95\n",
    "            local_k_sens_results=np.array(local_k_sens_results)\n",
    "            fitness_map[rank_key]=local_k_sens_results\n",
    "        results_k_sens+=local_k_sens_results\n",
    "    final_fitness = np.trapz(results_k_sens, list(range(0, len(results_k_sens))))\n",
    "    \n",
    "    return final_fitness,\n",
    "\n",
    "\n",
    "\n",
    "# Now define the genetic operators\n",
    "toolbox.register(\"evaluate\", evalIndividual)\n",
    "\n",
    "# toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"select\", tools.selDoubleTournament, fitness_size=3, parsimony_size=1.4, fitness_first=True) # it encourages simple solutions\n",
    "# NSGA-III requires a reference point set to guide the evolution into creating a uniform Pareto front in the objective space\n",
    "#reference_points = tools.uniform_reference_points(nobj=1, p=12)  \n",
    "#toolbox.register(\"select\", tools.selNSGA3, ref_points=reference_points)\n",
    "toolbox.register(\"mate\", gp.cxOnePoint) # in-place operation\n",
    "toolbox.register(\"mutate\", gp.mutNodeReplacement, pset=pset) # in-place operation\n",
    "toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=15))\n",
    "toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=15))\n",
    "\n",
    "\n",
    "# Finally, we define the main function in which the genetic algorithm will run\n",
    "def main_genetic_function(MAXGEN):\n",
    "    list_best_fitnesses = []\n",
    "    list_best_sols = []\n",
    "    # Mutation and crossover probabilities, and maximum number of individuals\n",
    "    CXPB, MUTPB = 0.7, 0.4 # cross 0.7, mut 0.4\n",
    "    processes=31\n",
    "    pop_size = processes*100 #10\n",
    "    \n",
    "    # Variable keeping track of the number of generation\n",
    "    g = 0\n",
    "    \n",
    "    # Create the population\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "        \n",
    "    # Determine the validation fitnesses\n",
    "    fitness_params = zip(pop, [g]*len(pop))\n",
    "    #     fitnesses = list(map(toolbox.evaluate, fitness_params))\n",
    "    \n",
    "    # mp version\n",
    "#     pool = multiprocessing.Pool(processes=processes)\n",
    "#     fitnesses = pool.map(toolbox.evaluate, fitness_params)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "    # sp version\n",
    "    fitnesses = [toolbox.evaluate(par) for  par in fitness_params]\n",
    "    \n",
    "    # Determine the best fitness\n",
    "    list_best_fitnesses.append(np.min(fitnesses))\n",
    "                         \n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # Determine the best individual in the population\n",
    "    best_sol_pop = min(pop, key=attrgetter(\"fitness\"))\n",
    "    list_best_sols.append(best_sol_pop)\n",
    "\n",
    "    # Evolve the population\n",
    "    fitnesses = [ind.fitness.values[0] for ind in pop]\n",
    "    \n",
    "    \n",
    "    print(\"-- Generation %i --\" % g)\n",
    "    length = len(pop)\n",
    "    mean = sum(fitnesses) / length\n",
    "    sum2 = sum(x*x for x in fitnesses)\n",
    "    std = abs(sum2 / length - mean**2)**0.5\n",
    "    print(\"Fitnesses: Min %s\" % round(min(fitnesses), 6), \"  Max %s\" % round(max(fitnesses), 6), \"  Avg %s\" % round(mean, 6), \"  Std %s\" % round(std, 6))\n",
    "    print('Best individual', best_sol_pop)\n",
    "    \n",
    "    # Begin the evolution\n",
    "    while g < MAXGEN:\n",
    "        # A new generation\n",
    "        g = g + 1\n",
    "        print(\"-- Generation %i --\" % g)\n",
    "        # Select the next generation individuals, having the same size as the previous generation\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals: this ensures that we don’t use a reference to the individuals but a completely independent instance\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "         # Apply crossover and mutation on the offspring\n",
    "        for i in range(1, len(offspring), 2): \n",
    "            if random.random() < CXPB:\n",
    "                offspring[i-1], offspring[i] = toolbox.mate(offspring[i-1], offspring[i])\n",
    "                del offspring[i-1].fitness.values\n",
    "                del offspring[i].fitness.values\n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitness_params = zip(invalid_ind, [g]*len(invalid_ind))\n",
    "                     \n",
    "        # mp version\n",
    "#         pool = multiprocessing.Pool(processes=processes)\n",
    "#         fitnesses = pool.map(toolbox.evaluate, fitness_params)\n",
    "#         pool.close()\n",
    "#         pool.join()     \n",
    "        \n",
    "        # sp version\n",
    "        fitnesses = [toolbox.evaluate(par) for par in fitness_params]\n",
    "    \n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        # Finally, replace the old population by the offspring, composed of some elements of the old population and some new elements\n",
    "        pop[:] = offspring\n",
    "        \n",
    "        # Determine the fitness vals\n",
    "        fitnesses= [ind.fitness.values[0] for ind in pop]\n",
    "        list_best_fitnesses.append(np.min(fitnesses))\n",
    "\n",
    "        # Determine best individual in the population \n",
    "        best_sol_pop = min(pop, key=attrgetter(\"fitness\"))\n",
    "        list_best_sols.append(best_sol_pop)\n",
    "                         \n",
    "        # Gather all the fitnesses in one list and print the stats (validation)\n",
    "        length = len(pop)\n",
    "        mean = sum(fitnesses) / length\n",
    "        sum2 = sum(x*x for x in fitnesses)\n",
    "        std = abs(sum2 / length - mean**2)**0.5\n",
    "        print(\"Fitnesses: Min %s\" % round(min(fitnesses), 6), \"  Max %s\" % round(max(fitnesses), 6), \"  Avg %s\" % round(mean, 6), \"  Std %s\" % round(std, 6))\n",
    "        print('Best individual', best_sol_pop)\n",
    "        write_pickle(zip(list_best_sols,list_best_fitnesses), 'genetic_partial_results.pickle')\n",
    "    \n",
    "    return list_best_sols, list_best_fitnesses\n",
    "\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    list_top_solutions, list_top_fitnesses = main_genetic_function(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimate rank map with genetic programming function\n",
    "\n",
    "# Load best genetic programming function\n",
    "# results = load_pickle('genetic_partial_results.pickle')\n",
    "# gp_func=toolbox.compile(expr=list(results)[0][0])\n",
    "\n",
    "def gp_func(ARG0,ARG1,ARG2):\n",
    "    return my_max(ARG1, my_max(my_pow(my_square(ARG0), my_add(ARG1, ARG1)), my_add(ARG0, ARG2)))\n",
    "\n",
    "\n",
    "\n",
    "ranks = {\n",
    "#          'angular':sensor_map_angular_ranks,\n",
    "         'euclidean':sensor_map_euclidean_ranks,\n",
    "         'manhattan':sensor_map_manhattan_ranks,\n",
    "         'chebyshev':sensor_map_chebyshev_ranks,\n",
    "        }\n",
    "\n",
    "sensor_map_old_weighted_ranks = {}\n",
    "for ind, sens in enumerate(all_sensors):\n",
    "    # prepare the combined rank\n",
    "    other_sens = sorted(set(all_sensors) - set([sens]))\n",
    "    sensor_corrs = []\n",
    "    for j in range(len(other_sens)):\n",
    "        sensor_corrs+=[gp_func(#sorted(ranks['angular'][sens])[j][1],\n",
    "                       (sorted(ranks['euclidean'][sens])[j][1]-min_euclidean)/(max_euclidean - min_euclidean),\n",
    "                       (sorted(ranks['manhattan'][sens])[j][1]-min_manhattan)/(max_manhattan - min_manhattan),\n",
    "                       (sorted(ranks['chebyshev'][sens])[j][1]-min_chebyshev)/(max_chebyshev - min_chebyshev))]\n",
    "        \n",
    "    # sort the correlations to obtain a rank from best to worst (higher distance)\n",
    "    sensor_corrs = zip(other_sens,sensor_corrs)\n",
    "    sensor_map_old_weighted_ranks[sens] = sorted(sensor_corrs, key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimating rank map\n",
    "\n",
    "# So, for each sensor, I now determine the rank of the other sensors,\n",
    "# based on the Kendall correlation value (from the most correlated, to the lowest)\n",
    "# Using training data only\n",
    "\n",
    "#!pip install saxpy\n",
    "# https://cs.gmu.edu/~jessica/SAX_DAMI_preprint.pdf\n",
    "# While it’s intuitive that larger alphabet sizes yield better results, there are diminishing returns\n",
    "# as a increases. If space is an issue, an alphabet size in the range 5 to 8 seems to be a good choice\n",
    "# that offers a reasonable balance between space and tightness of lower bound – each alphabet within\n",
    "# this range can be represented with just 3 bits. Increasing the alphabet size would require more\n",
    "# bits to represent each alphabet. \n",
    "\n",
    "    \n",
    "    \n",
    "from xgboost import XGBRegressor\n",
    "import importlib\n",
    "\n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "from saxpy.znorm import znorm\n",
    "from saxpy.sax import ts_to_string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import zlib\n",
    "import sys\n",
    "import shap\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Maps store the results from the best to the worst sensor, according to the specific metric\n",
    "sensor_map_kendall_ranks = {}\n",
    "sensor_map_pearson_ranks = {}\n",
    "sensor_map_spearman_ranks = {}\n",
    "sensor_map_euclidean_ranks = {}\n",
    "sensor_map_SAXCBD_ranks = {}\n",
    "sensor_map_SHAP_ranks = {}\n",
    "\n",
    "\n",
    "use_eval = False\n",
    "\n",
    "condition=True\n",
    "if use_eval:\n",
    "    condition=(all_training_data.year_week.isin(train_weeks))\n",
    "\n",
    "pbar = tqdm(total=len(all_sensors))\n",
    "for ind, sens in enumerate(all_sensors):\n",
    "    \n",
    "    sens_values = all_training_data[(all_training_data.node == sens) & condition]['temperature'].values\n",
    "    \n",
    "    sens_sax = ts_to_string(znorm(sens_values), cuts_for_asize(8))\n",
    "    sens_sax_compr = zlib.compress(sens_sax.encode())\n",
    "    sens_sax_compr_size = sys.getsizeof(sens_sax_compr)\n",
    "    \n",
    "    kendall_corrs = []\n",
    "    pearson_corrs = []\n",
    "    spearman_corrs = []\n",
    "    euclidean_dists = []\n",
    "    xgb_errors = []\n",
    "    saxcbd_comprs = []\n",
    "    \n",
    "    other_sensors = sorted(list(set(all_sensors) - set([sens])))\n",
    "    for other_sens in other_sensors: \n",
    "        other_ind = list(all_sensors).index(other_sens)\n",
    "        other_sens_values = all_training_data[(all_training_data.node == other_sens) & condition]['temperature'].values\n",
    "        \n",
    "        other_sens_sax = ts_to_string(znorm(other_sens_values), cuts_for_asize(8))\n",
    "        other_sens_sax_compr = zlib.compress(other_sens_sax.encode())\n",
    "        other_sens_sax_compr_size = sys.getsizeof(other_sens_sax_compr)\n",
    "        both_sens_sax_compr = zlib.compress((sens_sax + other_sens_sax).encode())\n",
    "        both_sens_sax_compr_size = sys.getsizeof(both_sens_sax_compr)\n",
    "        cbd_ratio = both_sens_sax_compr_size / (sens_sax_compr_size + other_sens_sax_compr_size) \n",
    "    \n",
    "        kendall_corrs.append(1./(1.+abs(kendalltau(sens_values, other_sens_values)[0])))\n",
    "        pearson_corrs.append(1./(1.+abs(np.corrcoef(sens_values, other_sens_values)[0, 1])))\n",
    "        spearman_corrs.append(1./(1.+abs(spearmanr(sens_values, other_sens_values)[0])))\n",
    "        euclidean_dists.append(np.linalg.norm(sens_values - other_sens_values))\n",
    "        \n",
    "        saxcbd_comprs.append(cbd_ratio)\n",
    "    \n",
    "    \n",
    "    # Compute Shapley values\n",
    "    train_xgb = pd.DataFrame()\n",
    "    for other_sens in other_sensors:\n",
    "        train_xgb[other_sens + '_temperature'] =  all_training_data[(all_training_data.node == other_sens) & condition]['temperature'].values\n",
    "\n",
    "    reg = XGBRegressor(n_estimators=100, n_jobs=31, random_state=42).fit(train_xgb, sens_values)\n",
    "\n",
    "    subsampled = shap.sample(train_xgb, 1000)\n",
    "    explainer = shap.TreeExplainer(reg, subsampled)\n",
    "    shap_values = explainer.shap_values(subsampled)\n",
    "    average_shaps = 1./(1.+np.average(abs(shap_values), axis=0))\n",
    "    feature_names = [x.split(\"_\")[0] for x in train_xgb.columns]\n",
    "    sensor_map_SHAP_ranks[sens] = [(x, y) for (y, x) in sorted(zip(average_shaps, feature_names), key=lambda pair: pair[0], reverse=False)]\n",
    "        \n",
    "    sensor_map_kendall_ranks[sens] = [(x,y) for y,x in sorted(zip(kendall_corrs, other_sensors), reverse=False)]\n",
    "    sensor_map_pearson_ranks[sens] = [(x,y) for y,x in sorted(zip(pearson_corrs, other_sensors), reverse=False)]\n",
    "    sensor_map_spearman_ranks[sens] = [(x,y) for y,x in sorted(zip(spearman_corrs, other_sensors), reverse=False)]\n",
    "    sensor_map_euclidean_ranks[sens] = [(x,y) for y,x in sorted(zip(euclidean_dists, other_sensors), reverse=False)]\n",
    "    sensor_map_SAXCBD_ranks[sens] = [(x,y) for y,x in sorted(zip(saxcbd_comprs, other_sensors), reverse=False)]\n",
    "    \n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "write_pickle(sensor_map_kendall_ranks, 'ranks/sensor_map_kendall_ranks.pickle')\n",
    "write_pickle(sensor_map_pearson_ranks, 'ranks/sensor_map_pearson_ranks.pickle')\n",
    "write_pickle(sensor_map_spearman_ranks, 'ranks/sensor_map_spearman_ranks.pickle')\n",
    "write_pickle(sensor_map_euclidean_ranks, 'ranks/sensor_map_euclidean_ranks.pickle')\n",
    "write_pickle(sensor_map_SAXCBD_ranks, 'ranks/sensor_map_SAXCBD_ranks.pickle')\n",
    "write_pickle(sensor_map_SHAP_ranks, 'ranks/sensor_map_SHAP_ranks.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_map_kendall_ranks = load_pickle('ranks/sensor_map_kendall_ranks.pickle')\n",
    "sensor_map_pearson_ranks = load_pickle('ranks/sensor_map_pearson_ranks.pickle')\n",
    "sensor_map_spearman_ranks = load_pickle('ranks/sensor_map_spearman_ranks.pickle')\n",
    "sensor_map_euclidean_ranks = load_pickle('ranks/sensor_map_euclidean_ranks.pickle')\n",
    "sensor_map_SAXCBD_ranks = load_pickle('ranks/sensor_map_SAXCBD_ranks.pickle')\n",
    "sensor_map_SHAP_ranks = load_pickle('ranks/sensor_map_SHAP_ranks.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing  ranks on models preditions\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "results_k_sens = pd.DataFrame(columns=['rank', 'predicted_sensor', 'n_discarded_sensors', 'ecdf_area_95th'])\n",
    "\n",
    "\n",
    "# must be metrics by which low values mean better values\n",
    "ranks = {\n",
    "#          'angular':sensor_map_angular_ranks,\n",
    "#          'angular_center':sensor_map_angular_center_ranks,\n",
    "         'euclidean':sensor_map_euclidean_ranks,\n",
    "         'manhattan':sensor_map_manhattan_ranks,\n",
    "         'chebyshev':sensor_map_chebyshev_ranks,\n",
    "#          'window':sensor_map_window_ranks,\n",
    "#          'center':sensor_map_center_ranks,\n",
    "         'gp_function':sensor_map_weighted_ranks,\n",
    "         'kendall': sensor_map_kendall_ranks,\n",
    "         'pearson':sensor_map_pearson_ranks,\n",
    "#          'spearman':sensor_map_spearman_ranks,  \n",
    "         'shap': sensor_map_SHAP_ranks,\n",
    "         'saxcbd': sensor_map_SAXCBD_ranks    \n",
    "}\n",
    "\n",
    "\n",
    "model = \"XGBoostRegression\"\n",
    "# LinearRegression   XGBoostRegression  \n",
    "\n",
    "pbar_outer = tqdm(total=len(ranks)*len(all_sensors))\n",
    "for key in ranks:\n",
    "    \n",
    "    curmap = ranks[key]\n",
    "    \n",
    "    #pbar = tqdm(total=len(list(counts_per_sensor.index)))\n",
    "    for ind, sens in enumerate(all_sensors):\n",
    "        sensor_corrs = curmap[sens]\n",
    "\n",
    "        sensor_corrs = sorted(sensor_corrs, key=lambda x: x[1], reverse=True) # in this way, we have the sensors from the worst to the best \n",
    "\n",
    "        #print(sensor_corrs)\n",
    "        \n",
    "        for k in range(0, len(all_sensors)-1): # from no discarded sensor to all but one discarded sensors\n",
    "            \n",
    "            ## preparing the dataset\n",
    "            sensors_to_avoid = [x for (x, _) in sensor_corrs[0:k]]\n",
    "            ref_sensors_here = sorted((set(all_sensors) - set(sensors_to_avoid)) - set([sens]))\n",
    "\n",
    "            ## extrapolate trainset and testset from traindata and testdata\n",
    "            predictors_cols = [col for col in sensor_cols[sens] if (not sens in col) and not any(sensor_to_avoid in col for sensor_to_avoid in sensors_to_avoid)] \n",
    "            X_train = sensor_trainset[sens][:,  [list(sensor_cols[sens]).index(x) for x in predictors_cols ]]\n",
    "            X_test = sensor_testset[sens][:, [list(sensor_cols[sens]).index(x) for x in predictors_cols ]]\n",
    "            y_train = sensor_trainset[sens][:, list(sensor_cols[sens]).index(sens+'_temperature')]\n",
    "            y_test = sensor_testset[sens][:, list(sensor_cols[sens]).index(sens+'_temperature')]\n",
    "\n",
    "            if model == 'LinearRegression':\n",
    "                reg = LinearRegression(n_jobs=31).fit(X_train, y_train)\n",
    "            elif model == 'XGBoostRegression':\n",
    "                reg = XGBRegressor(n_estimators=100, n_jobs=2, random_state=42).fit(X_train, y_train)\n",
    "            else:\n",
    "                assert False\n",
    "            \n",
    "            predictions = reg.predict(X_test)\n",
    "\n",
    "            errors = np.abs(y_test - predictions)\n",
    "\n",
    "            ecdf_x = np.sort(errors)\n",
    "            ecdf_y = np.arange(1, len(ecdf_x) + 1) / len(ecdf_x)\n",
    "            perc_95 = int(len(ecdf_x)*0.95)\n",
    "            ecdf_area_95 = np.trapz(ecdf_y[:perc_95], ecdf_x[:perc_95])\n",
    "\n",
    "            new_row = pd.DataFrame()\n",
    "            new_row['rank'] = [key]\n",
    "            new_row['predicted_sensor'] = [sens]\n",
    "            new_row['n_discarded_sensors'] = [k]\n",
    "            new_row['ecdf_area_95th'] = [ecdf_area_95]\n",
    "            results_k_sens = results_k_sens.append(new_row)\n",
    "\n",
    "        pbar_outer.update(1)\n",
    "pbar_outer.close()\n",
    "\n",
    "write_pickle(results_k_sens, 'results_k_sens_' + model + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = { \n",
    "#          'angular':sensor_map_angular_ranks,\n",
    "#          'angular_center':sensor_map_angular_center_ranks,\n",
    "         'euclidean':sensor_map_euclidean_ranks,\n",
    "         'manhattan':sensor_map_manhattan_ranks,\n",
    "         'chebyshev':sensor_map_chebyshev_ranks,\n",
    "#          'window':sensor_map_window_ranks,\n",
    "#          'center':sensor_map_center_ranks,\n",
    "         'gp_function':sensor_map_weighted_ranks,\n",
    "         'kendall': sensor_map_kendall_ranks,\n",
    "         'pearson':sensor_map_pearson_ranks,\n",
    "#          'spearman':sensor_map_spearman_ranks,\n",
    "         'shap': sensor_map_SHAP_ranks,\n",
    "         'saxcbd': sensor_map_SAXCBD_ranks    \n",
    "}\n",
    "\n",
    "rank_labels={\n",
    "     'euclidean': 'Euclidean',\n",
    "     'manhattan': 'Manhattan',\n",
    "     'chebyshev': 'Chebyshev',\n",
    "     'gp_function': 'GP_function',\n",
    "     'kendall': 'Kendall',\n",
    "     'pearson': 'Pearson',\n",
    "     'spearman' : 'Spearman',\n",
    "     'shap': 'SHAP',\n",
    "     'saxcbd': 'SAX-CBD'\n",
    "}\n",
    "\n",
    "\n",
    "model='XGBoostRegression'\n",
    "# LinearRegression\n",
    "# XGBoostRegression\n",
    "\n",
    "results_k_sens = load_pickle('results_k_sens_' + model + '.pickle')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "# plt.rcParams[\"font.size\"] = \"11\"\n",
    "plot_string = \"Area under the curve: \\n\"\n",
    "\n",
    "\n",
    "ranks_errors_list = []\n",
    "\n",
    "\n",
    "# get  list of methods ordered by error AUC\n",
    "maxval = -1\n",
    "for key in ranks:\n",
    "    subresults = results_k_sens.query(\"rank == @key\")\n",
    "    grouped = subresults.groupby(['n_discarded_sensors']).sum()\n",
    "    maxval = np.max(grouped['ecdf_area_95th']) if np.max(grouped['ecdf_area_95th']) > maxval else maxval\n",
    "    \n",
    "    auc = round(np.trapz(grouped['ecdf_area_95th'], list(range(0, len(grouped['ecdf_area_95th'])))), 2)\n",
    "    ranks_errors_list += [ (auc,key)]\n",
    "\n",
    "ranks_errors_list = sorted(ranks_errors_list, reverse=False)\n",
    "\n",
    "\n",
    "maxval = -1\n",
    "for key in [ k for (_,k) in ranks_errors_list ]:\n",
    "    subresults = results_k_sens.query(\"rank == @key\")\n",
    "    grouped = subresults.groupby(['n_discarded_sensors']).sum()\n",
    "    maxval = np.max(grouped['ecdf_area_95th']) if np.max(grouped['ecdf_area_95th']) > maxval else maxval\n",
    "    plt.plot(range(1,len(grouped['ecdf_area_95th'])+1), grouped['ecdf_area_95th'].tolist()[::-1], label=rank_labels[key])\n",
    "    \n",
    "    plot_string += \"  > \" + rank_labels[key] + \": \" + str(round(np.trapz(grouped['ecdf_area_95th'], list(range(0, len(grouped['ecdf_area_95th'])))), 2)) + \"\\n\"\n",
    "plt.xlabel(\"Number of selected sensors (from best to worst according to the rank)\")\n",
    "# plt.xlim(1,11)\n",
    "plt.ylabel(\"Sum of 95th percentile error (temperature)\")\n",
    "# plt.title(\"Performance of \" + model + \", discarding sensors based on training set ranks\")\n",
    "plt.legend(title=\"Ranking strategy\")\n",
    "plt.text(x=7.2, y=maxval-1.79, s=plot_string)\n",
    "plt.savefig('plots/Selected_sensor_results_k_sens_ProxyMetrics_' + model + '.pdf', bbox_inches='tight', pad_inches=0, dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the pearson ranks we estimate how many times a sensor is present in the first n_refs = 4 (elbow) positions\n",
    "\n",
    "results_best_sens = {}\n",
    "for s in all_sensors:\n",
    "    results_best_sens[s]=0\n",
    "\n",
    "n_refs = 11 \n",
    "\n",
    "# must be metrics by which low values mean better values\n",
    "ranks = {\n",
    "         'angular':sensor_map_angular_ranks,\n",
    "         'euclidean':sensor_map_euclidean_ranks,\n",
    "         'manhattan':sensor_map_manhattan_ranks,\n",
    "         'chebyshev':sensor_map_chebyshev_ranks,\n",
    "         'gp_function':sensor_map_weighted_ranks,\n",
    "         'pearson':sensor_map_pearson_ranks,\n",
    "         'shap': sensor_map_SHAP_ranks,\n",
    "         'saxcbd': sensor_map_SAXCBD_ranks    \n",
    "}\n",
    "\n",
    "distance='pearson'\n",
    "\n",
    "rank = ranks[distance]\n",
    "\n",
    "epsilon=0.1\n",
    "sensors_error = results_k_sens.query('rank==@distance').groupby('predicted_sensor')[['ecdf_area_95th']].sum().values\n",
    "sensors_weight = np.subtract(1, np.divide(sensors_error, np.max(sensors_error+epsilon)))\n",
    "\n",
    "for i,sens in enumerate(all_sensors):\n",
    "    sensors_rank = rank[sens]\n",
    "    sensors_rank = sorted(sensors_rank, key=lambda x: x[1], reverse=False) # in this way, we have the sensors from the best to the worst\n",
    "\n",
    "    sensors_rank = [x for (x, _) in sensors_rank[0:n_refs]]\n",
    "\n",
    "     # first j sensors in the rank\n",
    "    for j in range(n_refs):\n",
    "        results_best_sens[sensors_rank[j]]+=(n_refs-j)*sensors_weight[i][0]\n",
    "        \n",
    "# print(results_best_sens)\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "results_best_sens=sorted([x for x in zip(results_best_sens.keys(), results_best_sens.values())], key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "keys=[x[0] for x in results_best_sens]\n",
    "values=[x[1] for x in results_best_sens]\n",
    "\n",
    "\n",
    "kn = KneeLocator(range(len(values)), values, curve='convex', direction='decreasing')\n",
    "print('elbow:', kn.knee+1)\n",
    "\n",
    "plt.bar(range(len(values)), height=values)\n",
    "plt.plot(range(len(values)), values, color='b')\n",
    "\n",
    "plt.xticks(range(len(keys)), keys, rotation=35, ha=\"right\",rotation_mode=\"anchor\")\n",
    "plt.ylabel('Weighted Borda count vote')# 'Borda count vote considering '+str(n_refs)+' positions'\n",
    "\n",
    "plt.savefig('plots/best_ranked_sensors_'+ distance +'.pdf', dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
